---
title: "Random Forest Model Builder"
author: "Scott Coffin"
date: "1/19/2021"
output: 
  html_document:
    code_folding: hide
    theme: sandstone
    toc: yes
    toc_float: yes
    toc_depth: 4
    number_sections: true
  word_document:
    toc: yes
---
```{r global-options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      warning=FALSE, message=FALSE, echo = FALSE)
```
---

This script uses the randomForest package to build machine learning models, instead of the Caret package in the other RMD scripts in this folder. 

# Setup
```{r setup}
# Load packages
library(tidyverse) #General everything
library(RColorBrewer)
library(ggplot2) #General plotting
library(ggrepel) #For adding text labels that repel away from data points
library(calecopal) #Color palette
library(shiny) #Runs shiny
library(shinythemes) #Shiny theme for the page
library(shinyWidgets) #Widgets
library(scales) #SSD - Use the percent format
library(reshape2) #Overview tab - melts bars together
library(ssdtools) #SSD package
library(DT) #Build HTML data tables
#library(plotly) #Make plots interactive
library(viridis) #Colors
library(scales) #To use "percent" function
library(tigerstats) #turns things into percents
library(ggbeeswarm) #plot all points
library(fitdistrplus, exclude = 'select') #alt SSD 
library(RColorBrewer) #colors
library(pheatmap) #pretty heat maps
library(rpart)  #for trees
#library(rattle)    # Fancy tree plot This is a difficult library to install (https://gist.github.com/zhiyzuo/a489ffdcc5da87f28f8589a55aa206dd) 
library(rpart.plot)             # Enhanced tree plots
library(RColorBrewer)       # Color selection for fancy tree plot
library(party)                  # Alternative decision tree algorithm
# library(partykit)               # Convert rpart object to BinaryTree
library(pROC)   #for ROC curves
library(uwot) #umap
library(ISLR)  #for the Carseat Data
library(lme4) #general linearized mixed model GLMM
library(quantregForest)
library(caret)
library(tidyverse)
library(tidymodels)
library(skimr)
library(sf)
library(ggspatial)
library(nhdplusTools)
library(patchwork)
library(Metrics)
library(gt)
library(randomForest)
library(DALEX) #explanatory model analysis

```
## Data import
```{r data import}
# Load finalized dataset.
# load tox data
aoc_z <- readRDS(file = "Tox Data/aoc_z.Rda") 

# Master dataset for SSDs
aoc_z <- aoc_z %>% # start with Heili's altered dataset (no filtration for terrestrial data)
  # environment category data tidying.
  mutate(environment.noNA = replace_na(environment, "Not Reported")) %>% # replaces NA to better relabel.
  mutate(env_f = factor(environment.noNA, levels = c("Marine", "Freshwater", "Terrestrial", "Not Reported"))) 
 
# final cleanup and factoring  
aoc_z$Species <- as.factor(paste(aoc_z$genus,aoc_z$species)) #must make value 'Species" (uppercase)
aoc_z$Group <- as.factor(aoc_z$organism.group) #must make value "Group"
aoc_z$Group <- fct_explicit_na(aoc_z$Group) #makes sure that species get counted even if they're missing a group

# subset data to selected variables

multiVar <- aoc_z %>% dplyr::select(#doi, size.category, 
                                    size_f,
                                    size.length.um.used.for.conversions, 
                                    shape, 
                                    polymer, 
                                    particle.volume.um3, 
                                    density.mg.um.3, 
                                    organism.group,
                                    environment, 
                                    bio.org, #biological level of organization
                                    #af.time, #assessment factor based on exposure time
                                    treatments, #number of doses (no including control)
                                    effect, #yes no
                                    effect_f,
                                   # effect_10,
                                    size_f,
                                    exposure.duration.d, 
                                    exposure.route, #Factor
                                    organism.group, #factor
                                    media.temp, #numeric
                                    lvl1_f, #endpoints
                                    lvl2_f, #endpoints
                                   # lvl3, 
                                     dose.mg.L.master, 
                                    sex, #factor
                                    media.ph, #numeric
                                    media.sal.ppt, #numeric
                                    dose.particles.mL.master,
                                   effect.metric, #NOEC LOEC
                                    functional.group, #factor
                                    charge, #positive or negatibe
                                    zetapotential.mV, # numeric   
                                   max.size.ingest.mm,#max ingestible size
                                   acute.chronic_f,
                                  # dose.mg.L.master.AF.noec,
                                  # dose.particles.mL.master.AF.noec,
                                   dose.um3.mL.master,
                                   max.size.ingest.mm, #maximum ingestible size range
                                   effect.score) %>%  #1 = minor, 2 = photosynthesis, feeding, 3 = growth, chlorophyll content, 4 = reproduction, 5  = population growth, 6 = survival
  filter(!size_f == "Not Reported") %>%   #take out not reported 
                                 #  max.size.ingest.mm) %>%  #max ingestible size
  filter(!size_f == "Not Reported")  #take out not reported 

#recode variables
# multiVar <- multiVar %>% mutate(effect_10 = case_when(
#     effect == "Y" ~ 1,
#     effect == "N" ~ 0
#   ))# %>% 
#   #mutate_all(is.character, ~as.factor())
 
```


# Analysis
## Data Exploration
### Completeness by Size
```{r completeness heatmap 1}
CompletenessSummary_size <- multiVar %>%
  group_by(size_f) %>% 
     summarise_all((name = ~sum(is.na(.))/length(.))) %>% 
  mutate(across(is.numeric,~round(., 2))) %>% 
  mutate(across(is.numeric, ~100 *(1 -.)))
CompletenessSummary_size
```

```{r}
require(pheatmap)
#convert to matrix and transpose
transposed_size <- as.data.frame(t(as.matrix(CompletenessSummary_size[2:25]))) %>% #1 is category, 2-6 are variables
  arrange('1nm < 100nm')
#reassign column names
colnames(transposed_size) <- c("1nm < 100nm", "100nm < 1µm", "1µm < 100µm", "100µm < 1mm", "1mm < 5mm")
#transposedTag
#format as matrix
MissingMatrix_size <- data.matrix(transposed_size)
#build heatmap
pheatmap(MissingMatrix_size,
                           main = "Data Completeness by Size Bin", #title
                           fontsize = 12,
                           cluster_rows = FALSE, cluster_cols = FALSE,#disable dendrograms
                           display_numbers = TRUE,
                           treeheight_row = 0, treeheight_col = 0, #keeps clustering after dropping dendrograms
                           col = rev(brewer.pal(n = 9, name = "PuBu"))) #blue color scheme with 9 colors)

```
### Completeness by Environment
```{r completeness heatmap 2}
CompletenessSummary_environment <- multiVar %>%
  filter(!environment == "NA") %>% 
  group_by(environment) %>% 
     summarise_all((name = ~sum(is.na(.))/length(.))) %>% 
  mutate(across(is.numeric,~round(., 2))) %>% 
  mutate(across(is.numeric, ~100 *(1 -.)))
CompletenessSummary_environment
```

```{r}
require(pheatmap)
#convert to matrix and transpose
transposed_environment <- as.data.frame(t(as.matrix(CompletenessSummary_environment[2:25])))
#reassign column names
colnames(transposed_environment) <- c("Freshwater", "Marine", "Terrestrial")
#transposedTag
#format as matrix
MissingMatrix_environment <- data.matrix(transposed_environment)
#build heatmap
pheatmap(MissingMatrix_environment,
                           main = "Data Completeness by environment", #title
                           fontsize = 12,
                           cluster_rows = FALSE, cluster_cols = FALSE,#disable dendrograms
                           display_numbers = TRUE,
                           treeheight_row = 0, treeheight_col = 0, #keeps clustering after dropping dendrograms
                           col = rev(brewer.pal(n = 9, name = "PuBu"))) #blue color scheme with 9 colors)

```

## Stepwise Regression

```{r}
## Estimate an OLS Regression
fitols <- lm(effect_10 ~ size.length.um.used.for.conversions + particle.volume.um3 + exposure.duration.d + media.temp + dose.particles.mL.master, 
             na.action = na.omit, 
             data = multiVar)
summary(fitols)
```



```{r}
require(reshape2)
multiVar %>% 
  dplyr::select(size.length.um.used.for.conversions, particle.volume.um3, exposure.duration.d ,media.temp, dose.particles.mL.master) %>% 
  melt() %>%  #convert wide to long
   mutate_if(~is.numeric(.) && (.) > 0, log10) %>% 
  ggplot(aes(x = value)) + 
  stat_density() + 
  facet_wrap(~variable, scales = "free")
```


<!-- # ```{r} -->
<!-- # # cdplot(as.factor(effect) ~ particle.volume.um3, data = multiVar, -->
<!-- #        main = "Conditional Density Plot of ", xlab = "Dose", ylab = "Effect") -->
<!-- # ``` -->





## Random Forest
### Recursive Partitioning And Regression Trees

The rpart algorithm works by splitting the dataset recursively, which means that the subsets that arise from a split are further split until a predetermined termination criterion is reached.  At each step, the split is made based on the independent variable that results in the largest possible reduction in heterogeneity of the dependent (predicted) variable.

It is important to note that the algorithm works by making the best possible choice at each particular stage, without any consideration of whether those choices remain optimal in future stages. That is, the algorithm makes a locally optimal decision at each stage. It is thus quite possible that such a choice at one stage turns out to be sub-optimal in the overall scheme of things.  In other words,  the algorithm does not find a globally optimal tree.
```{r}
#trim data so effect is always known
multiVar_sub <- multiVar %>% 
  drop_na(effect_10)

# Split data into training and test sets
set.seed(42)
multiVar_sub[,"train"] <- ifelse(runif(nrow(multiVar_sub)) < 0.8, 1, 0)
# Separate trainig and test sets
trainSet <- multiVar_sub[multiVar_sub$train==1,]
testSet <- multiVar_sub[multiVar_sub$train==0,]
#get column index of train flag
trainColNum <- grep("train", names(trainSet))
# Remove train flag column from train and test sets
trainSet <- trainSet[, -trainColNum]
testSet <- testSet[, -trainColNum]
```
Make a classification tree.

```{r, echo = FALSE}
#get column index of predicted variable in dataset
typeColNum <- grep("effect",names(multiVar_sub))

#build tree
require(rpart)
set.seed(15097)
t1 <- rpart(effect ~ size.length.um.used.for.conversions + particle.volume.um3 + exposure.duration.d + media.temp + dose.particles.mL.master,
            method = "class", #classification because response is discrete
            control = rpart.control(minbucket = 20, cp=0.008), #requires that there be at least 19 cases (responded + nonrespondents) in the final grouping of variable values of the terminal node of the tre..
            data = trainSet)
print(t1, digits=4)
```


Plot an interpretable tree.
```{r, echo = FALSE}
require(rpart.plot)
cols <- ifelse(t1$frame$yval == 1, "gray50", "black")
prp(t1, main="Tree for Effect",
    extra=106, # display prob of survival and percent of obs
    nn=TRUE, # display node numbers
    fallen.leaves=TRUE, # put leaves on the bottom of page
    branch=.5, # change angle of branch lines
    faclen=0, # do not abbreviate factor levels
    trace=1, # print automatically calculated cex
    shadow.col="gray", # shadows under the leaves
    branch.lty=1, # draw branches using solid lines
    branch.type=5, # branch lines width = weight(frame$wt), no. of cases here
    split.cex=1.2, # make split text larger than node text
    split.prefix="is ", # put "is " before split text
    split.suffix="?", # put "?" after split text
    col=cols, border.col=cols, # cols[2] if survived
    split.box.col="lightgray", # lightgray split boxes (default is white)
    split.border.col="darkgray", # darkgray border on split boxes
    split.round=0.5) # round the split box corners a tad
```
Next we see how good the model is by seeing how it fares against the test data.
```{r}
t1_predict <- predict(t1, newdata = testSet[,-typeColNum],
                      type="class")
mean(t1_predict==testSet$effect)
# [1] 0.7094088
#confusion matrix
table(pred=t1_predict,true=testSet$effect)
```
```{r}
par(mfrow=c(1,2)) # two plots on one page
#plot approximate R-squared and relative error for different splits (2 plots). labels are only appropriate for the "anova" method.
rsq.rpart(t1)
```


Next, we prune the tree using the cost complexity criterion. Basically, the intent is to see if a shallower subtree can give us comparable results. If so, we’d be better of choosing the shallower tree because it reduces the likelihood of overfitting.

As described earlier, we choose the appropriate pruning parameter (aka cost-complexity parameter) \alpha by picking the value that results in the lowest prediction error. Note that all relevant computations have already been carried out by R when we built the original tree (the call to rpart in the code above). All that remains now is to pick the value of \alpha:

#### Pruning

```{r}
#cost-complexity pruning
printcp(t1)
```
It is clear from the above, that the lowest cross-validation error (xerror in the table) occurs for \alpha =0.008 (this is CP in the table above).   One can find CP programatically like so:
```{r}
# get index of CP with lowest xerror
opt <- which.min(t1$cptable[,"xerror"])
#get its value
cp <- t1$cptable[opt, "CP"]
```
Next, we prune the tree based on this value of CP:

```{r}
# # prune the tree
# pt1 <- prune(t1,cp)
# pt1<- prune(t1, cp= t1$cptable[which.min(t1$cptable[,"xerror"]),"CP"])
# 
# # plot the pruned tree
# plot(pt1, uniform=TRUE,
#    main="Pruned Classification Tree");text(pt1, use.n=TRUE, all=TRUE, cex=.8)
# 
# #post(pfit, file = "c:/ptree.ps",
#  #  title = "Pruned Classification Tree for Kyphosis")
```
```{r}
# #find proportion of correct predictions using test set
# t1_pruned_predict <- predict(pt1, testSet, type="class")
# mean(t1_pruned_predict == testSet$effect)
# #
```
This is not an improvement over an unprunend tree.. We need to check that this holds up for different training and test sets. This is easily done by creating multiple random partitions of the dataset and checking the efficacy of pruning for each. To do this efficiently, I’ll create a function that takes the training fraction, number of runs (partitions) and the name of the dataset as inputs and outputs the proportion of correct predictions for each run. It also optionally prunes the tree. 

```{r}
# <!-- # #function to do multiple runs -->
# <!-- # multiple_runs_classification <- function(train_fraction,n,dataset,prune_tree=FALSE){ -->
# <!-- # fraction_correct <- rep(NA,n) -->
# <!-- # set.seed(42) -->
# <!-- # for (i in 1:n){ -->
# <!-- #   dataset[,"train"] <- ifelse(runif(nrow(dataset))<0.8,1,0) -->
# <!-- #   trainColNum <- grep("train",names(dataset)) -->
# <!-- #   typeColNum <- grep("effect",names(dataset)) -->
# <!-- #   trainSet <- dataset[dataset$train==1,-trainColNum] -->
# <!-- #   testSet <- dataset[dataset$train==0,-trainColNum] -->
# <!-- #   rpart_model <- rpart(effect~ size.length.um.used.for.conversions + particle.volume.um3 + exposure.duration.d + media.temp + dose.particles.mL.master,data = trainSet, method="class") -->
# <!-- # if(prune_tree==FALSE) { -->
# <!-- #   rpart_test_predict <- predict(rpart_model,testSet[,-typeColNum],type="class") -->
# <!-- #   fraction_correct[i] <- mean(rpart_test_predict==testSet$effect) -->
# <!-- #   }else{ -->
# <!-- #     opt <- which.min(rpart_model$cptable[,"xerror"]) -->
# <!-- #     cp <- rpart_model$cptable[opt, "CP"] -->
# <!-- #     pruned_model <- prune(rpart_model,cp) -->
# <!-- #     rpart_pruned_predict <- predict(pruned_model,testSet[,-typeColNum],type="class") -->
# <!-- #     fraction_correct[i] <- mean(rpart_pruned_predict == testSet$effect) -->
# <!-- #   } -->
# <!-- #   } -->
# <!-- # return(fraction_correct) -->
# <!-- # } -->
# <!-- # ``` -->
#  
# <!-- # Note that in the above,  I have set the default value of the prune_tree to FALSE, so the function will execute the first branch of the if statement unless the default is overridden. -->
# <!-- #  -->
# <!-- # OK, so let’s do 50 runs with and without pruning, and check the mean and variance of the results for both sets of runs. -->
# <!-- # ```{r} -->
# <!-- # #50 runs, no pruning -->
# <!-- # unpruned_set <- multiple_runs_classification(0.8,50,multiVar_sub) -->
# <!-- # mean(unpruned_set) -->
# <!-- # #[1] 0.7261882 -->
# <!-- # sd(unpruned_set) -->
# <!-- # #[1] 0.01554347 -->
# <!-- # #50 runs, with pruning -->
# <!-- # pruned_set <- multiple_runs_classification(0.8,50,multiVar_sub,prune_tree=TRUE) -->
# <!-- # mean(pruned_set) -->
# <!-- # #[1] 0.7261875 -->
# <!-- # sd(pruned_set) -->
# <!-- # #[1] 0.01552285 -->
# <!-- # ``` -->
```


### CForest
```{r run model}
require(party)
crf <- cforest(as.factor(effect_10) ~ size.length.um.used.for.conversions + particle.volume.um3 + exposure.duration.d +
                 media.temp + dose.particles.mL.master,
               controls = cforest_control(ntree = 500,
                                          mincriterion = qnorm(0.8), 
                                          trace = TRUE), # adds project bar because it's very slow
               data = multiVar_sub)

crf
```

```{r fit results 1}
train2 <- multiVar_sub %>% dplyr::select(size.length.um.used.for.conversions,
                                  particle.volume.um3,exposure.duration.d,media.temp,dose.particles.mL.master)
train3 <-multiVar_sub %>% dplyr::select(size.length.um.used.for.conversions,
                                  particle.volume.um3,exposure.duration.d,media.temp,dose.particles.mL.master,
                                  effect_10)

fitted <- predict(crf, train2, OOB = TRUE, type ="response")
#rpart.prob <- predict(t1, newdata=imputedSmalls.requested.voluntary,type="prob")

misClasificError <- mean(fitted != train3$effect_10)
print(paste('Training Accuracy', 1 - misClasificError))
```

```{r}
print(crf)
```

<!-- # ```{r} -->
<!-- # #Obtaining predicted probabilites for Test data -->
<!-- # tree.probs=predict(crf, -->
<!-- #                  newdata = train2, -->
<!-- #                  type="prob") -->
<!-- #  -->
<!-- # #Calculate ROC curve -->
<!-- # rocCurve.tree <- roc(train3$effect_10,tree.probs[,'1']) -->
<!-- # #plot the ROC curve -->
<!-- # plot(rocCurve.tree,col=c(4)) -->
<!-- # ``` -->


Alternative ROC Curve
```{r}
require(pROC)
predicted <- predict(crf, train2, OOB=TRUE, type= "response")
auc(as.numeric(train3$effect_10), as.numeric(predicted))
#Calculate ROC curve
rocCurve.tree <- roc(train3$effect_10,as.numeric(predicted))
#plot the ROC curve
plot(rocCurve.tree,col=c(4))
```


```{r}
# compute in-sample results
caret::confusionMatrix(fitted,as.factor(train3$effect_10))
```

```{r}
#plot feature importance
cforestImpPlot <- function(x) {
  cforest_importance <<- v <- varimp(x)
  dotchart(v[order(v)])
}

importancePlot <- cforestImpPlot(crf)
importancePlot
```


<!-- # ```{r} -->
<!-- # crf.prob <- matrix(unlist(crfsrvy.prob), ncol=2, byrow=TRUE) -->
<!-- # apply(crf.prob,2,mean) -->
<!-- # #[1] 0.2524514 0.7475486 -->
<!-- # tab <- round(cbind(by(rpart.prob[,2], INDICES=t1$where, mean), by(crf.prob[,2], INDICES=t1$where, mean)), 4) -->
<!-- # colnames(tab) <- c("rpart", "cforest") -->
<!-- # kable(tab) -->
<!-- # ``` -->

#GLMM
```{r}
#tutorial https://aosmith.rbind.io/2020/08/20/simulate-binomial-glmm/
require(lme4)

mod = glm(effect_f ~ size.length.um.used.for.conversions + polymer + particle.volume.um3 + density.mg.um.3 + organism.group + bio.org + treatments + effect_f + exposure.duration.d + exposure.route + lvl1_f + dose.mg.L.master, 
            data = aoc_z,
            family = binomial(link = "logit") )
mod
```

```{r}
bin_glmm_fun = function(n_sites = 10,
                        b0 = 0,
                        b1 = 1.735,
                        num_samp = 50,
                        site_var = 0.5) {
     site = rep(LETTERS[1:n_sites], each = 2)
     plot = paste(site, rep(1:2, times = n_sites), sep = "." )
     treatment = rep( c("treatment", "control"), times = n_sites)
     dat = data.frame(site, plot, treatment)           
     
     site_eff = rep( rnorm(n = n_sites, mean = 0, sd = sqrt(site_var) ), each = 2)
     
     log_odds = with(dat, b0 + b1*(treatment == "treatment") + site_eff)
     prop = plogis(log_odds)
     dat$num_samp = num_samp
     dat$y = rbinom(n = n_sites*2, size = num_samp, prob = prop)
     
     glmer(cbind(y, num_samp - y) ~ treatment + (1|site),
           data = dat,
           family = binomial(link = "logit") )
}


set.seed(16)
bin_glmm_fun()

```


# Random Forest Modelling

## Preparation
```{r}
library(quantregForest)
library(caret)
library(tidyverse)
library(tidymodels)
library(skimr)
library(sf)
library(ggspatial)
library(nhdplusTools)
library(patchwork)
library(Metrics)
library(gt)
```

```{r}
aoc_z_2 <- aoc_z %>% 
  mutate_if(is.character, as.factor)# %>%
  # mutate(effect_10 = case_when( #convert ordinal to numeric
  #    effect_f == "Yes" ~ 1,
  #    effect_f == "No" ~ 0))

#choose relevant predictors and log-transform
multiVar_small_log <- aoc_z_2 %>% 
  dplyr::select(size_f,
                ID, #row number ID for split/joining by study
                doi, #need to split studies
                size.length.um.used.for.conversions, shape, polymer, particle.volume.um3, density.mg.um.3, organism.group, bio.org, treatments, effect_f, exposure.duration.d, exposure.route, lvl1_f, dose.mg.L.master) %>% 
  mutate_if(~is.numeric(.) && (.) > 0, log10) %>% 
  drop_na() %>%  #drop missing
 mutate(effect_10 = case_when( #convert ordinal to numeric
     effect_f == "Y" ~ 1,
     effect_f == "N" ~ 0
   ))# %>% 
 #  select(-effect_f)
skim(multiVar_small_log)
```

```{r}
#no log transform for comparison
multiVar_small <- aoc_z %>% 
  dplyr::select(size_f, size.length.um.used.for.conversions, shape, polymer, particle.volume.um3, density.mg.um.3, organism.group, bio.org, treatments, effect_f, life.stage,
                exposure.duration.d, lvl1_f, dose.mg.L.master, dose.particles.mL.master, dose.um3.mL.master)# %>% 
  #drop_na() #%>%  #drop missing
# mutate(effect_10 = case_when( #convert ordinal to numeric
#     effect == "Y" ~ 1,
#     effect == "N" ~ 0
#   )) %>% 
  # select(-effect)

#ensure completeness
skim(multiVar_small)
```

##NON-LOG TRANSFORMED
### Missing Value Imputation by Rough Fix
```{r}
#rough fix
multiVar_small_roughfix <- na.roughfix(multiVar_small)

# Create calibration and validation splits with tidymodels initial_split() function.
set.seed(4)
multiVar_small_split <- multiVar_small_roughfix %>%
  initial_split(prop = 0.75, strata = polymer) # splits data into training and testing set.
# default is 3/4ths split (but 75% training, 25% testing).
# Stratification (strata) = grouping training/testing sets by region, state, etc.
# Using the "strata" call ensures the number of data points in the training data is equivalent to the proportions in the original data set. (Strata below 10% of the total are pooled together.)

# Create a training data set with the training() function
# Pulls from training and testing sets created by initial_split()
multiVar_small_train <- training(multiVar_small_split)
multiVar_small_test <- testing(multiVar_small_split)
# Examine the environment to be sure # of observations looks like the 75/25 split. 3199:1066.

# Create a separate dataset of available IDs that were not used in the training dataset.
notTrain <- aoc %>% # all COMIDS from StreamCat data, sampled or not
  filter(!ID %in% aoc$ID) # Removing sites used to train the model. n = 140,097

count_optimized <- paste0('n = ',nrow(multiVar_small_roughfix))

skim(multiVar_small_roughfix)
```
#### Kitchen Sink Model
#### LINEAR DATA
```{r}
# Step Three - Kitchen Sink model -----------------------------------------
# Random forest -- 
# a decision tree model, using predictors to answer dichotomous questions to create nested splits.
# no pruning happens - rather, multiple trees are built (the forest) and then you are looking for consensus across trees
# training data goes down the tree and ends up in a terminal node.
# if testing data goes down the same route, then this upholds our conclusions. Or, if it goes awry, this allows us to look for patterns in how it goes awry.

set.seed(2) # assures the data pulled is random, but sets it for the run below (makes outcome stable)
myrf <- randomForest(y = multiVar_small_roughfix$effect_f, # dependent variable
  x = multiVar_small_roughfix %>%
    dplyr::select(-effect_f), # selecting all predictor variables
  importance = T, # how useful is a predictor in predicting values (nothing causal)
  proximity = T, 
  na.action = na.roughfix,
  ntrees = 100) # 500 trees default. 

myrf # examine the results.
```
~20% error rate. Let's compare this with the same model with log-transformed values.


##### Predictor Selector/Recursive Feature Elimination
```{r}
# Using caret to select the best predictors
# What are the parameters you want to use to run recursive feature elimination (rfe)?
my_ctrl <- rfeControl(functions = rfFuncs,
                      method = "cv",
                      verbose = FALSE,
                      returnResamp = "all")

# rfe = recursive feature elimination
# THIS STEP TAKES FOR-EV-ER!!!
set.seed(22)
my_rfe <- rfe(y = multiVar_small_roughfix$effect_f, # set dependent variable
              x = multiVar_small_roughfix %>% 
                dplyr::select(-effect_f),
              rfeControl = my_ctrl,
               size = c(1:2, 4, 6, 8, 10, 12, 14, 15))#,
             # na.action = na.roughfix())

# sets how many variables are in the overall model
#               # I have 13 total possible variables, so I've chosen increments of 3 to look at.
#               rfeControl = my_ctrl,
#               na.action = na.roughfix,
#               testX = multiVar_small_test %>% dplyr::select(-effect_f),
#               testY = multiVar_small_test$effect_f)

# can you make your model even simpler?
# the following will pick a model with the smallest number of predictor variables based on the tolerance ("tol") that you specify (how much less than the best are you willing to tolerate?)

#inspect
my_rfe
```

```{r}
trellis.par.set(caretTheme())
#visualize
plot(my_rfe,type = c("g", "o"))
```

```{r}
my_size <- pickSizeTolerance(my_rfe$results, metric = "Accuracy", tol = 5, maximize = TRUE)
# higher tol (~10) gives you less variables
# lower tol (~1) gives you more variables - "I'd like the simplest model within 1% of the best model."
pickVars(my_rfe$variables, size = my_size)
```
The pickSizeTolerance determines the absolute best value then the percent difference of the other points to this value. This approach can produce good results for many of the tree based models, such as random forest, where there is a plateau of good performance for larger subset sizes. For trees, this is usually because unimportant variables are infrequently used in splits and do not significantly affect performance.

Just use best predictors.
```{r}
set.seed(2) # assures the data pulled is random, but sets it for the run below (makes outcome stable)
myrf_optimized <- randomForest(y = multiVar_small_train$effect_f, # dependent variable
  x = multiVar_small_train %>%
    dplyr::select(c(organism.group, exposure.duration.d, lvl1_f, dose.um3.mL.master)), # selecting all predictor variables
  importance = T, # how useful is a predictor in predicting values (nothing causal)
  proximity = T, 
  na.action = na.roughfix,
  ntrees = 100) # 500 trees default. 

myrf_optimized # examine the results.
```
```{r}
varImpPlot(myrf_optimized)
```
```{r}

optimized_fitted <- predict(myrf_optimized, multiVar_small_test %>% dplyr::select(-effect_f), OOB = TRUE, type ="response")
misClasificError_optimized <- mean(optimized_fitted != multiVar_small_test$effect_f)
accuracy_optimized <- paste0('Accuracy: ', round(100*(1 - misClasificError_optimized), 2), '%')
accuracy_optimized
```

```{r}
df1 <- aoc_z %>% 
  dplyr::select(c(organism.group, exposure.duration.d, lvl1_f, dose.um3.mL.master, effect_10)) %>% 
  drop_na()

m1 <- glm(effect_10 ~ organism.group + exposure.duration.d + lvl1_f + dose.um3.mL.master, 
   data = df1, na.action = na.omit, family = "binomial")
summary(m1)
```

```{r}
require(ggeffects)
m1_g <- ggeffects::ggpredict(m1, terms = "dose.um3.mL.master")
plot(m1_g)
```
### Missing Value Imputation by RF

The algorithm starts by imputing NAs using na.roughfix. Then randomForest is called with the completed data. The proximity matrix from the randomForest is used to update the imputation of the NAs. For continuous predictors, the imputed value is the weighted average of the non-missing obervations, where the weights are the proximities. For categorical predictors, the imputed value is the category with the largest average proximity. This process is iterated iter times.

Note: Imputation has not (yet) been implemented for the unsupervised case. Also, Breiman (2003) notes that the OOB estimate of error from randomForest tend to be optimistic when run on the data matrix with imputed values.
```{r}
# impute values
#drop NA's in response
multiVar_small_noNa <- multiVar_small %>% drop_na(effect_f)

#impute
set.seed(111)
multiVar_small_rfImpute <- rfImpute(data = multiVar_small_noNa, 
                                    effect_f ~., #response value, cannot contain NA's
                                    iter = 4,
                                    ntree =75)

# Create calibration and validation splits with tidymodels initial_split() function.
set.seed(4)
multiVar_small_split_imputed <- multiVar_small_rfImpute %>%
  initial_split(prop = 0.75, strata = polymer) # splits data into training and testing set.
# default is 3/4ths split (but 75% training, 25% testing).
# Stratification (strata) = grouping training/testing sets by region, state, etc.
# Using the "strata" call ensures the number of data points in the training data is equivalent to the proportions in the original data set. (Strata below 10% of the total are pooled together.)

# Create a training data set with the training() function
# Pulls from training and testing sets created by initial_split()
multiVar_small_train_imputed <- training(multiVar_small_split_imputed)
multiVar_small_test_imputed <- testing(multiVar_small_split_imputed)
# Examine the environment to be sure # of observations looks like the 75/25 split. 3199:1066.

# Create a separate dataset of available IDs that were not used in the training dataset.
notTrain <- aoc %>% # all COMIDS from StreamCat data, sampled or not
  filter(!ID %in% aoc$ID) # Removing sites used to train the model. n = 140,097

count_optimized_imputed <- paste0('n = ',nrow(multiVar_small_rfImpute))

skim(multiVar_small_rfImpute)
```
#### Kitchen Sink Model
#### LINEAR DATA
```{r}
# Step Three - Kitchen Sink model -----------------------------------------
# Random forest -- 
# a decision tree model, using predictors to answer dichotomous questions to create nested splits.
# no pruning happens - rather, multiple trees are built (the forest) and then you are looking for consensus across trees
# training data goes down the tree and ends up in a terminal node.
# if testing data goes down the same route, then this upholds our conclusions. Or, if it goes awry, this allows us to look for patterns in how it goes awry.

set.seed(2) # assures the data pulled is random, but sets it for the run below (makes outcome stable)
myrf <- randomForest(y = multiVar_small_rfImpute$effect_f, # dependent variable
  x = multiVar_small_rfImpute %>%
    dplyr::select(-effect_f), # selecting all predictor variables
  importance = T, # how useful is a predictor in predicting values (nothing causal)
  proximity = T, 
  ntrees = 100) # 500 trees default. 

myrf # examine the results.
```
~20% error rate. Let's compare this with the same model with log-transformed values.
```{r}
plot(myrf, log = "y", main = "Random Forest for Imputed Data")
```


##### Predictor Selector/Recursive Feature Elimination
```{r}
# Using caret to select the best predictors
# What are the parameters you want to use to run recursive feature elimination (rfe)?
my_ctrl <- rfeControl(functions = rfFuncs,
                      method = "cv",
                      verbose = FALSE,
                      returnResamp = "all")

# rfe = recursive feature elimination
# THIS STEP TAKES FOR-EV-ER!!!
set.seed(22)
my_rfe_imputed <- rfe(y = multiVar_small_rfImpute$effect_f, # set dependent variable
              x = multiVar_small_rfImpute %>% 
                dplyr::select(-effect_f),
              rfeControl = my_ctrl,
               size = c(1:2, 4, 6, 8, 10, 12, 14, 15))#,
             # na.action = na.rfImpute())

# sets how many variables are in the overall model
#               # I have 13 total possible variables, so I've chosen increments of 3 to look at.
#               rfeControl = my_ctrl,
#               na.action = na.rfImpute,
#               testX = multiVar_small_test %>% dplyr::select(-effect_f),
#               testY = multiVar_small_test$effect_f)

# can you make your model even simpler?
# the following will pick a model with the smallest number of predictor variables based on the tolerance ("tol") that you specify (how much less than the best are you willing to tolerate?)

#inspect
my_rfe_imputed
```

```{r}
trellis.par.set(caretTheme())
#visualize
plot(my_rfe_imputed,type = c("g", "o"))
```

```{r}
my_size <- pickSizeTolerance(my_rfe_imputed$results, metric = "Accuracy", tol = 5, maximize = TRUE)
# higher tol (~10) gives you less variables
# lower tol (~1) gives you more variables - "I'd like the simplest model within 1% of the best model."
pickVars(my_rfe_imputed$variables, size = my_size)
```
The pickSizeTolerance determines the absolute best value then the percent difference of the other points to this value. This approach can produce good results for many of the tree based models, such as random forest, where there is a plateau of good performance for larger subset sizes. For trees, this is usually because unimportant variables are infrequently used in splits and do not significantly affect performance.

Just use best predictors.
```{r}
set.seed(2) # assures the data pulled is random, but sets it for the run below (makes outcome stable)
myrf_optimized_imputed <- randomForest(y = multiVar_small_train$effect_f, # dependent variable
  x = multiVar_small_train %>%
    dplyr::select(c(organism.group, exposure.duration.d, lvl1_f, dose.um3.mL.master)), # selecting all predictor variables
  importance = T, # how useful is a predictor in predicting values (nothing causal)
  proximity = T, 
  ntrees = 100) # 500 trees default. 

myrf_optimized_imputed # examine the results.
```
```{r}
varImpPlot(myrf_optimized_imputed)
```
```{r}

optimized_imputed_fitted <- predict(myrf_optimized_imputed, multiVar_small_test_imputed %>% dplyr::select(-effect_f), OOB = TRUE, type ="response")
misClasificError_optimized_imputed <- mean(optimized_imputed_fitted != multiVar_small_test_imputed$effect_f)

accuracy_optimized_imputed <- paste0('Accuracy: ', round(100*(1 - misClasificError_optimized_imputed), 2), '%')

accuracy_optimized_imputed
```

####LOG DATA

```{r}
#log subset
## First split data by DOI, then re-join other data
set.seed(4)
doi_split <- multiVar_small_log %>% 
  dplyr::select(doi) %>% 
  unique() %>% 
  initial_split(prop = 0.65)
#split just by doi 
doi_train <- training(doi_split)
doi_test <- testing(doi_split)

train_full <- left_join(doi_train, multiVar_small_log, by = "doi") %>% 
  dplyr::select(-c(doi, ID, effect_10)) %>% 
  droplevels()

test_full <- left_join(doi_test, multiVar_small_log, by = "doi") %>% 
    dplyr::select(-c(doi, ID, effect_10)) %>% 
  droplevels()

#inspect proportion in test and train
nrow(test_full)
nrow(train_full)
```
Now that we've split the data by study and ensured a good proportion in test and train, let's run the model.

```{r}
set.seed(2) # assures the data pulled is random, but sets it for the run below (makes outcome stable)
myrf_log <- randomForest(y = train_full$effect_f, # dependent variable
  x = train_full %>%
    dplyr::select(-effect_f), # selecting all predictor variables
  importance = T, # how useful is a predictor in predicting values (nothing causal)
  proximity = T, 
  ntrees = 100) # 500 trees default. 

myrf_log # examine the results.
```
<!-- # ```{r fit result 2} -->
<!-- # #fix levels -->
<!-- # #levels(test_full$polymer) <- levels(train_full$polymer) -->
<!-- # levels(test_full$shape) <- levels(train_full$shape) -->
<!-- # levels(test_full$lvl1_f) <- levels(train_full$lvl1_f) -->
<!-- # #levels(test_full$organism.group) <- levels(train_full$organism.group) -->
<!-- # -->
<!-- # fitted <- predict(myrf_log, test_full %>% dplyr::select(-effect_f), OOB = TRUE, type ="response") -->
<!-- # misClasificError <- mean(fitted != test_full$effect_f) -->
<!-- # print(paste('Training Accuracy', 1 - misClasificError)) -->
<!-- # ``` -->


No performance enhancement with log-transformed values.

## Continuous Predictors
Repeat with continous variables whenever possible, and max of 8 predictors.
```{r}
skim(multiVar)
```
```{r}
acute <- aoc_z %>% 
  filter(acute.chronic_f == "Acute") %>% 
  filter(!environment == "Terrestrial") %>% 
  filter(bio.org == "organism") %>% 
  filter(lvl1_f == "Fitness") %>% 
  filter(!exposure.route == "food") %>% 
  filter(!polymer == "Not Reported") %>% 
  filter(!shape == "Not Reported") %>% 
  dplyr::select(c(doi, dose.mg.L.master, organism.group, size.length.um.used.for.conversions, polymer, shape, effect.score, dose.particles.mL.master, effect_f)) %>%   
  mutate_if(~is.numeric(.) && (.) > 0, log10) %>% 
  drop_na() %>% 
  droplevels()

count_acute <- paste0('n = ',nrow(acute))

skim(acute)
```
With a complete dataset for just acute studies in aquatic organisms with aqueous route of exposure, we are left with 453 complete cases with 6 predictor variables, 1 response variable (effect y/n), and an ID. Let's determine if we have enough data for ML. 
```{r}
exp(1)^6
```
$e^6$ is less than n (453), so we may proceed.

```{r}
## First split data by DOI, then re-join other data
set.seed(4)
acute_doi_split <- acute %>% 
  dplyr::select(doi) %>% 
  unique() %>% 
  initial_split(prop = 0.65)
#split just by doi 
acute_doi_train <- training(acute_doi_split)
acute_doi_test <- testing(acute_doi_split)

acute_train <- left_join(acute_doi_train, acute, by = "doi") %>% 
  dplyr::select(-doi) %>% 
  droplevels()

acute_test <- left_join(acute_doi_test, acute, by = "doi") %>% 
  dplyr::select(-doi) %>% 
  droplevels()

#inspect proportion in test and train
nrow(acute_test)
nrow(acute_train)
```

```{r}
# Create calibration and validation splits with tidymodels initial_split() function.
# set.seed(4)
# acute_split <- acute %>%
#   initial_split(prop = 0.75, strata = polymer) # splits data into training and testing set.
# # default is 3/4ths split (but 75% training, 25% testing).
# # Stratification (strata) = grouping training/testing sets by region, state, etc.
# # Using the "strata" call ensures the number of data points in the training data is equivalent to the proportions in the original data set. (Strata below 10% of the total are pooled together.)
# 
# # Create a training data set with the training() function
# # Pulls from training and testing sets created by initial_split()
# acute_train <- training(acute_split)
# acute_test <- testing(acute_split)
# # Examine the environment to be sure # of observations looks like the 75/25 split. 3199:1066.

# Random forest -- 
set.seed(2) # assures the data pulled is random, but sets it for the run below (makes outcome stable)
acuterf <- randomForest(y = acute_train$effect_f, # dependent variable
  x = acute_train %>%
    dplyr::select(-effect_f), # selecting all predictor variables
  importance = T, # how useful is a predictor in predicting values (nothing causal)
  proximity = T, 
  ntrees = 100) # 500 trees default. 

acuterf # examine the results.
```

```{r}
plot(acuterf)
# model performance appears to improve most at ~75 trees
```

```{r}
varImpPlot(acuterf)
# displays which variables are most important
# helps to winnow down list of predictors
# recommended to weigh left pane more
# right pane also shows how evenly things split based on the list of predictors
# values close to 0 can be dropped, but don't have to be
```
Dose, organism group, exposure duration are important.

```{r}
importance <- acuterf$importance
#View(importance)
# displays the data plotted in the plot above
```

### Performance and Validation


```{r fit results 3}
# fix levels
levels(acute_test$polymer) <- levels(acute_train$polymer)
levels(acute_test$shape) <- levels(acute_train$shape)
levels(acute_test$organism.group) <- levels(acute_train$organism.group)

fitted <- predict(acuterf, 
                  newdata = acute_test %>% dplyr::select(-effect_f), 
                  OOB = TRUE, type ="response")

misClasificError_acute <- mean(fitted != acute_test$effect_f)

accuracy_acute <- paste0('Accuracy: ', round(100*(1 - misClasificError_acute), 2), '%')
accuracy_acute
```

Alternative ROC Curve
```{r eval=FALSE, include=FALSE}
require(pROC)
predicted <- predict(acuterf, acute_test %>%  dplyr::select(-effect_f),
                       OOB=TRUE, type= "response")
#Calculate ROC curve
rocCurve.tree <- roc(as.numeric(acute_test$effect_f),as.numeric(predicted))

##gplot
# rocks <- roc()

#plot the ROC curve
plot(rocCurve.tree,col=c(4))
```
#### Chronic
```{r}
chronic <- aoc_z %>% 
  filter(acute.chronic_f == "Chronic") %>% 
  filter(!environment == "Terrestrial") %>% 
  filter(bio.org == "organism") %>% 
  filter(lvl1_f == "Fitness") %>% 
  filter(!exposure.route == "food") %>% 
  dplyr::select(c(doi, dose.mg.L.master, organism.group, size.length.um.used.for.conversions, polymer, shape, effect.score, dose.particles.mL.master, effect_f)) %>%
  mutate_if(~is.numeric(.) && (.) > 0, log10) %>% 
  drop_na() %>% 
  droplevels()

count_chronic <- paste0('n = ',nrow(chronic))

skim(chronic)
```
With a complete dataset for just acute studies in aquatic organisms with aqueous route of exposure, we are left with 453 complete cases with 6 predictor variables, 1 response variable (effect y/n), and an ID. Let's determine if we have enough data for ML. 
```{r}
exp(1)^6
```
$e^6$ is less than n (453), so we may proceed.
```{r}
## First split data by DOI, then re-join other data
set.seed(4)
chronic_doi_split <- chronic %>% 
  dplyr::select(doi) %>% 
  unique() %>% 
  initial_split(prop = 0.85)
#split just by doi 
chronic_doi_train <- training(chronic_doi_split)
chronic_doi_test <- testing(chronic_doi_split)

chronic_train <- left_join(chronic_doi_train, chronic, by = "doi") %>% 
  dplyr::select(-doi) %>% 
  droplevels()

chronic_test <- left_join(chronic_doi_test, chronic, by = "doi") %>% 
  dplyr::select(-doi) %>% 
  droplevels()

#inspect proportion in test and train
nrow(chronic_test)
nrow(chronic_train)
```

```{r}
# # Create calibration and validation splits with tidymodels initial_split() function.
# set.seed(4)
# chronic_split <- chronic %>%
#   initial_split(prop = 0.75, strata = polymer) # splits data into training and testing set.
# # default is 3/4ths split (but 75% training, 25% testing).
# # Stratification (strata) = grouping training/testing sets by region, state, etc.
# # Using the "strata" call ensures the number of data points in the training data is equivalent to the proportions in the original data set. (Strata below 10% of the total are pooled together.)
# 
# # Create a training data set with the training() function
# # Pulls from training and testing sets created by initial_split()
# chronic_train <- training(chronic_split)
# chronic_test <- testing(chronic_split)
# # Examine the environment to be sure # of observations looks like the 75/25 split. 3199:1066.

# Random forest -- 
set.seed(2) # assures the data pulled is random, but sets it for the run below (makes outcome stable)
chronicrf <- randomForest(y = chronic_train$effect_f, # dependent variable
  x = chronic_train %>%
    dplyr::select(-effect_f), # selecting all predictor variables
  importance = T, # how useful is a predictor in predicting values (nothing causal)
  proximity = T, 
  ntrees = 100) # 500 trees default. 

chronicrf # examine the results.
```

```{r}
plot(chronicrf)
# model performance appears to improve most at ~75 trees
```

```{r}
varImpPlot(chronicrf)
# displays which variables are most important
# helps to winnow down list of predictors
# recommended to weigh left pane more
# right pane also shows how evenly things split based on the list of predictors
# values close to 0 can be dropped, but don't have to be
```
Dose, organism group, exposure duration are important.

```{r}
importance <- chronicrf$importance
#View(importance)
# displays the data plotted in the plot above
```

```{r fit results 4}
# fix levels
levels(chronic_test$polymer) <- levels(chronic_train$polymer)
levels(chronic_test$shape) <- levels(chronic_train$shape)
levels(chronic_test$organism.group) <- levels(chronic_train$organism.group)

fitted <- predict(chronicrf, chronic_test %>% 
                    dplyr::select(-effect_f), 
                  OOB = TRUE, type ="response")

misClasificError_chronic <- mean(fitted != chronic_test$effect_f)

accuracy_chronic <- paste0('Accuracy: ', round(100*(1 - misClasificError_chronic), 2), '%')
accuracy_chronic
```

#### All Data
```{r}
all <- aoc_z %>% 
  #filter(acute.chronic_f == "Chronic") %>% 
  filter(!environment == "Terrestrial") %>% 
  filter(bio.org == "organism") %>% 
  filter(lvl1_f == "Fitness") %>% 
  filter(!exposure.route == "food") %>% 
  dplyr::select(c(size.length.um.used.for.conversions, shape, polymer, particle.volume.um3, organism.group, dose.mg.L.master, effect.score, lvl2_f, effect_f, doi)) %>%
  mutate_if(~is.numeric(.) && (.) > 0, log10) %>% 
  drop_na()

count_all <- paste0('n = ',nrow(all))

skim(all)
```
With a complete dataset for just organismal fitness studies in aquatic organisms with aqueous route of exposure, we are left with 1510 complete cases with 8 predictor variables, and 1 response variable (effect y/n). Let's determine if we have enough data for ML. 
```{r}
exp(1)^7
```
$e^6$ is less than n (453), so we may proceed.

```{r}
## First split data by DOI, then re-join other data
set.seed(4)
all_doi_split <- all %>% 
  dplyr::select(doi) %>% 
  unique() %>% 
  initial_split(prop = 0.7)
#split just by doi 
all_doi_train <- training(all_doi_split)
all_doi_test <- testing(all_doi_split)

all_train <- left_join(all_doi_train, all, by = "doi") %>% 
  dplyr::select(-doi) %>% 
  droplevels()

all_test <- left_join(all_doi_test, all, by = "doi") %>% 
  dplyr::select(-doi) %>% 
  droplevels()

#inspect proportion in test and train
nrow(all_test)
nrow(all_train)
```

```{r}
# Create calibration and validation splits with tidymodels initial_split() function.
set.seed(4)
# all_split <- all %>%
#   initial_split(prop = 0.75, strata = polymer) # splits data into training and testing set.
# # default is 3/4ths split (but 75% training, 25% testing).
# # Stratification (strata) = grouping training/testing sets by region, state, etc.
# # Using the "strata" call ensures the number of data points in the training data is equivalent to the proportions in the original data set. (Strata below 10% of the total are pooled together.)
# 
# # Create a training data set with the training() function
# # Pulls from training and testing sets created by initial_split()
# all_train <- training(all_split)
# all_test <- testing(all_split)
# # Examine the environment to be sure # of observations looks like the 75/25 split. 3199:1066.

# Random forest -- 
set.seed(2) # assures the data pulled is random, but sets it for the run below (makes outcome stable)
allrf <- randomForest(y = all_train$effect_f, # dependent variable
  x = all_train %>%
    dplyr::select(-effect_f), # selecting all predictor variables
  importance = T, # how useful is a predictor in predicting values (nothing causal)
  proximity = T, 
  ntrees = 100) # 500 trees default. 

allrf # examine the results.
```

```{r}
plot(allrf)
# model performance appears to improve most at ~75 trees
```

```{r}
varImpPlot(allrf)
# displays which variables are most important
# helps to winnow down list of predictors
# recommended to weigh left pane more
# right pane also shows how evenly things split based on the list of predictors
# values close to 0 can be dropped, but don't have to be
```
Dose, organism group, exposure duration are important.

```{r}
importance <- allrf$importance
#View(importance)
# displays the data plotted in the plot above
```

```{r fit results 5}
levels(all_test$polymer) <- levels(all_train$polymer)
levels(all_test$shape) <- levels(all_train$shape)
levels(all_test$organism.group) <- levels(all_train$organism.group)

fitted <- predict(allrf, all_test %>% 
                    dplyr::select(-effect_f), 
                  OOB = TRUE, type ="response")

misClasificError_all <- mean(fitted != all_test$effect_f)
accuracy_all <- paste0('Accuracy: ', round(100*(1 - misClasificError_all), 2), '%')
accuracy_all
```

#### NO FILTERS
```{r}
nofilter <- aoc_z %>% 
  #filter(acute.chronic_f == "Chronic") %>% 
  ##filter(!environment == "Terrestrial") %>% 
  #filter(bio.org == "organism") %>% 
  #filter(lvl1_f == "Fitness") %>% 
  #filter(!exposure.route == "food") %>% 
  filter(!shape == "Not Reported") %>% 
  filter(!polymer == "Not Reported") %>% 
  dplyr::select(c(doi, dose.mg.L.master, organism.group, size.length.um.used.for.conversions, polymer, shape,
                  #effect.score, 
                  dose.particles.mL.master, 
                  effect_f, 
                  particle.volume.um3, 
                  exposure.duration.d,
                  lvl1_f)) %>%
  droplevels() %>% 
  mutate_if(~is.numeric(.) && (.) > 0, log10) %>% 
  drop_na()

count_nofilter <- paste0('n = ',nrow(nofilter))

skim(nofilter)
```
With a complete dataset for just acute studies in aquatic organisms with aqueous route of exposure, we are left with 453 complete cases with 6 predictor variables, 1 response variable (effect y/n), and an ID. Let's determine if we have enough data for ML. 
```{r}
exp(1)^8
```
$e^6$ is less than n (453), so we may proceed.


```{r}
## First split data by DOI, then re-join other data
set.seed(4)
nofilter_doi_split <- nofilter %>% 
  dplyr::select(doi) %>% 
  unique() %>% 
  initial_split(prop = 0.84)
#split just by doi 
nofilter_doi_train <- training(nofilter_doi_split)
nofilter_doi_test <- testing(nofilter_doi_split)

nofilter_train <- left_join(nofilter_doi_train, nofilter, by = "doi") %>% 
  dplyr::select(-doi) %>% 
  droplevels()

nofilter_test <- left_join(nofilter_doi_test, nofilter, by = "doi") %>% 
  dplyr::select(-doi) %>% 
  droplevels()

#inspect proportion in test and train
nrow(nofilter_test)
nrow(nofilter_train)
```

```{r}
# Create calibration and validation splits with tidymodels initial_split() function.
# set.seed(4)
# nofilter_split <- nofilter %>%
#   initial_split(prop = 0.75, strata = polymer) # splits data into training and testing set.
# # default is 3/4ths split (but 75% training, 25% testing).
# # Stratification (strata) = grouping training/testing sets by region, state, etc.
# # Using the "strata" cnofilter ensures the number of data points in the training data is equivalent to the proportions in the original data set. (Strata below 10% of the total are pooled together.)
# 
# # Create a training data set with the training() function
# # Pulls from training and testing sets created by initial_split()
# nofilter_train <- training(nofilter_split)
# nofilter_test <- testing(nofilter_split)
# # Examine the environment to be sure # of observations looks like the 75/25 split. 3199:1066.

# Random forest -- 
set.seed(2) # assures the data pulled is random, but sets it for the run below (makes outcome stable)
nofilterrf <- randomForest(y = nofilter_train$effect_f, # dependent variable
  x = nofilter_train %>%
    dplyr::select(-effect_f), # selecting nofilter predictor variables
  importance = T, # how useful is a predictor in predicting values (nothing causal)
  proximity = T, 
  ntrees = 100) # 500 trees default. 

nofilterrf # examine the results.
```

```{r}
plot(nofilterrf)
# model performance appears to improve most at ~75 trees
```

```{r}
varImpPlot(nofilterrf)
# displays which variables are most important
# helps to winnow down list of predictors
# recommended to weigh left pane more
# right pane also shows how evenly things split based on the list of predictors
# values close to 0 can be dropped, but don't have to be
```
Dose, organism group, exposure duration are important.

```{r}
importance <- nofilterrf$importance
#View(importance)
# displays the data plotted in the plot above
```

```{r fit results 6}
#fix levels
levels(nofilter_test$polymer) <- levels(nofilter_train$polymer)
levels(nofilter_test$shape) <- levels(nofilter_train$shape)
levels(nofilter_test$lvl1_f) <- levels(nofilter_train$lvl1_f)
levels(nofilter_test$organism.group) <- levels(nofilter_train$organism.group)

x1 <- nofilter_test %>% dplyr::select(-effect_f)
fitted <- predict(nofilterrf, 
                  x1,
                  OOB = TRUE, type ="response")

misClasificError_nofilter <- mean(fitted != nofilter_test$effect_f)
accuracy_nofilter <- paste0('Accuracy: ', round(100*(1 - misClasificError_nofilter), 2), '%')
accuracy_nofilter
```


#### ROC Curves
Prep for plotting.
```{r}
###CHRONIC
chronicpredictions <- as.data.frame(predict(chronicrf, chronic_test %>% dplyr::select(-effect_f), type = "prob"))
# predict class and then attach test class
chronicpredictions$predict <- names(chronicpredictions)[1:2][apply(chronicpredictions[,1:2], 1, which.max)]
chronicpredictions$observed <- chronic_test$effect_f

####ACUTE
predictions <- as.data.frame(predict(acuterf, acute_test%>% dplyr::select(-effect_f), type = "prob"))
# predict class and then attach test class
predictions$predict <- names(predictions)[1:2][apply(predictions[,1:2], 1, which.max)]
predictions$observed <- acute_test$effect_f
head(predictions)

#### all
allpredictions <- as.data.frame(predict(allrf, all_test%>% dplyr::select(-effect_f), type = "prob"))
# predict class and then attach test class
allpredictions$predict <- names(allpredictions)[1:2][apply(allpredictions[,1:2], 1, which.max)]
allpredictions$observed <- all_test$effect_f
head(allpredictions)


###nofilter
nofilterpredictions <- as.data.frame(predict(nofilterrf, nofilter_test%>% dplyr::select(-effect_f), type = "prob"))
# predict class and then attach test class
nofilterpredictions$predict <- names(nofilterpredictions)[1:2][apply(nofilterpredictions[,1:2], 1, which.max)]
nofilterpredictions$observed <- nofilter_test$effect_f

###nofilterOptimized
nofilter.optimizedpredictions <- as.data.frame(predict(myrf_optimized, multiVar_small_test %>%  dplyr::select(-effect_f), type = "prob"))
# predict class and then attach test class
nofilter.optimizedpredictions$predict <- names(nofilter.optimizedpredictions)[1:2][apply(nofilter.optimizedpredictions[,1:2], 1, which.max)]
nofilter.optimizedpredictions$observed <- multiVar_small_test$effect_f

###nofilterOptimizedImputed
nofilter.optimized.imputedpredictions <- as.data.frame(predict(myrf_optimized_imputed, multiVar_small_test_imputed %>%  dplyr::select(-effect_f), type = "prob"))
# predict class and then attach test class
nofilter.optimized.imputedpredictions$predict <- names(nofilter.optimized.imputedpredictions)[1:2][apply(nofilter.optimized.imputedpredictions[,1:2], 1, which.max)]
nofilter.optimized.imputedpredictions$observed <- multiVar_small_test_imputed$effect_f
```
Plot.
```{r}
require(ggdark)
# 1 ROC curve, yes vs no for acute
roc.acute <- roc(ifelse(predictions$observed=="Y", "Y", "N"), as.numeric(predictions$Y))

#chronic
roc.chronic <- roc(ifelse(chronicpredictions$observed=="Y", "Y", "N"), as.numeric(chronicpredictions$Y))
#all
roc.all <- roc(ifelse(allpredictions$observed=="Y", "Y", "N"), as.numeric(allpredictions$Y))

#nofilter
roc.nofilter <- roc(ifelse(nofilterpredictions$observed=="Y", "Y", "N"), as.numeric(nofilterpredictions$Y))

#no filter (optimized)
roc.nofilter.optimized <- roc(ifelse(nofilter.optimizedpredictions$observed=="Y", "Y", "N"), as.numeric(nofilter.optimizedpredictions$Y))

#no filter (optimized; imputed)
roc.nofilter.optimized.imputed <- roc(ifelse(nofilter.optimized.imputedpredictions$observed=="Y", "Y", "N"), as.numeric(nofilter.optimized.imputedpredictions$Y))

##make ROC curves

#all
allROC <- ggroc(roc.all, col = "yellow") + 
  labs(title = "Chronic and Acute",
       subtitle = paste0(accuracy_all,', ',count_all)) + 
  dark_theme_bw()

#acute
acuteROC <- ggroc(roc.acute, col = "green") + 
  labs(title = "Acute",
       subtitle = paste0(accuracy_acute,', ',count_acute)) + 
  dark_theme_bw()

#chronic
chronicROC <- ggroc(roc.chronic, col = "blue") + 
  labs(title = "Chronic",
       subtitle = paste0(accuracy_chronic,', ',count_chronic)) + #auto label
  dark_theme_bw()

#no filter
nofilterROC <- ggroc(roc.nofilter, col = "red3") + 
  labs(title = "Entire Dataset",
       subtitle = paste0(accuracy_nofilter,', ',count_nofilter)) + 
  dark_theme_bw()

#optimized (rough fix)
nofilteroptimizedROC <- ggroc(roc.nofilter.optimized, col = "orangered") + 
  labs(title = "Entire Dataset (optimized)",
       subtitle = paste0(accuracy_optimized,', ',count_optimized)) + 
  dark_theme_bw()

#optimized (multiple imputation)
nofilteroptimizedimputedROC <- ggroc(roc.nofilter.optimized.imputed, col = "orange") + 
  labs(title = "Entire Dataset (optimized; imputed)",
       subtitle = paste0(accuracy_optimized_imputed,', ',count_optimized_imputed)) + 
  dark_theme_bw()

#arrange together and print
require(gridExtra)
grid.arrange(nofilterROC, nofilteroptimizedROC, nofilteroptimizedimputedROC,allROC, acuteROC, chronicROC,
             ncol = 3)
```
ROC curves may also be visualized together
```{r}
require(pROC)
require(tidyverse)
require(ggdark)
require(ggsci)
ggroc(list(all = roc.nofilter, optimized = roc.nofilter.optimized, optimized.imputed = roc.nofilter.optimized.imputed, organisms = roc.all, acute = roc.acute, chronic = roc.chronic), aes = c("linetype", "color")) +
  labs(title = "ROC Curves for Aquatic Toxicity Random Forest",
       subtitle = "n = 4615",
       color = "Dataset",
       linetype = "Dataset") +
   scale_color_tron() +
  # theme_bw(base_size = 20)
 dark_theme_bw(base_size = 20)# +
 theme(plot.title.position = element_text(hjust = 0.5),
     plot.subtitle.position = element_text(hjust = 0.5))
```
# Multiple Models with DALEX

```{r}
#no log transform for comparison
optimized <- aoc_z %>% 
  dplyr::select(c(organism.group, exposure.duration.d, lvl1_f, dose.um3.mL.master, life.stage, bio.org, polymer, shape, size.length.um.used.for.conversions, treatments, effect)) %>% 
  drop_na() %>%  #drop missing
mutate_if(~is.numeric(.) && (.) > 0, log10) %>% 
  mutate(effect_10 = case_when( #convert ordinal to numeric
      effect == "Y" ~ 1,
      effect == "N" ~ 0
    )) %>%
  mutate(effect_10 = factor(effect_10)) %>% 
  dplyr::select(-(effect))

#ensure completeness
skim(optimized)
```

Alt Method
https://raw.githack.com/pbiecek/DALEX_docs/master/vignettes/DALEX_caret.html#3_classification_use_case_-_wine_data
```{r}
# Create calibration and validation splits with tidymodels initial_split() function.
set.seed(4)
optimized_split <- optimized %>%
  initial_split(prop = 0.75)
# default is 3/4ths split (but 75% training, 25% testing).

# Create a training data set with the training() function
# Pulls from training and testing sets created by initial_split()
optimized_train <- training(optimized_split)
optimized_test <- testing(optimized_split)
# Examine the environment to be sure # of observations looks like the 75/25 split. 3199:1066.

classif_rf <- train(effect_10~., data = optimized_train, method = "rf", ntree = 100, tuneLength = 1)

classif_glm <- train(effect_10~., data = optimized_train, method = "glm", family = "binomial")

classif_svm <- train(effect_10~., data = optimized_train, method = "svmRadial", prob.model = TRUE, tuneLength = 1)

# In this case we consider the differences between observed class and predicted probabilities to be residuals. So, we have to provide custom predict function which takes two arguments: model and newdata and returns a numeric vector with probabilities.

# create custom predict function
p_fun <- function(object, newdata){
  predict(object, newdata = newdata, type="prob")[,11]}

# make response variable numeric binary vector
yTest <- as.numeric(as.character(optimized_test$effect_10))

explainer_classif_rf <- DALEX::explain(classif_rf, label = "rf",
                                       data = optimized_test, 
                                       y = yTest,
                                       predict_function = p_fun, 
                                       verbose = FALSE)

explainer_classif_glm <- DALEX::explain(classif_glm, label = "glm", 
                                        data = optimized_test, y = yTest,
                                        predict_function = p_fun,
                                        verbose = FALSE)

explainer_classif_svm <- DALEX::explain(classif_svm,  label = "svm", 
                                        data = optimized_test, y = yTest,
                                        predict_function = p_fun,
                                        verbose = FALSE)

```
### Model Performance
```{r}
mp_classif_rf <- model_performance(explainer_classif_rf)
mp_classif_glm <- model_performance(explainer_classif_glm)
mp_classif_svm <- model_performance(explainer_classif_svm)

plot(mp_classif_rf, mp_classif_glm, mp_classif_svm)
```
#### Boxplot
```{r}
plot(mp_classif_rf, mp_classif_glm, mp_classif_svm, geom = "boxplot")
```
### Variable Importance
```{r}
vi_classif_rf <- model_parts(explainer_classif_rf, loss_function = loss_root_mean_square)
vi_classif_glm <- model_parts(explainer_classif_glm, loss_function = loss_root_mean_square)
vi_classif_svm <- model_parts(explainer_classif_svm, loss_function = loss_root_mean_square)

plot(vi_classif_rf, vi_classif_glm, vi_classif_svm)
```
#### Partial Dependence Plot
```{r}
pdp_classif_rf  <- model_profile(explainer_classif_rf, variable = "dose.um3.mL.master", type = "partial")
pdp_classif_glm  <- model_profile(explainer_classif_glm, variable = "dose.um3.mL.master", type = "partial")
pdp_classif_svm  <- model_profile(explainer_classif_svm, variable = "dose.um3.mL.master", type = "partial")

plot(pdp_classif_rf, pdp_classif_glm, pdp_classif_svm)
```
#### Accumulated Local Effects Plot

Accumulated local effects31 describe how features influence the prediction of a machine learning model on average. ALE plots are a faster and unbiased alternative to partial dependence plots (PDPs).


```{r}
ale_classif_rf  <- model_profile(explainer_classif_rf, variable = "alcohol", type = "accumulated")
ale_classif_glm  <- model_profile(explainer_classif_glm, variable = "alcohol", type = "accumulated")
ale_classif_svm  <- model_profile(explainer_classif_svm, variable = "alcohol", type = "accumulated")

plot(ale_classif_rf, ale_classif_glm, ale_classif_svm)
```

```{r}
sessionInfo()
```

# Interpretable Tree
Make a classification tree.

```{r, echo = FALSE}
#get column index of predicted variable in dataset
typeColNum <- grep("effect",names(all))

#build tree
require(rpart)
set.seed(15097)
t1 <- rpart(effect_f ~ size.length.um.used.for.conversions + shape + polymer + organism.group + particle.volume.um3 + dose.mg.L.master + effect.score,
            method = "class", #classification because response is discrete
            control = rpart.control(minbucket = 20, cp=0.008), #requires that there be at least 19 cases (responded + nonrespondents) in the final grouping of variable values of the terminal node of the tre..
            data = all_train)
print(t1, digits=4)
```


Plot an interpretable tree.
```{r, echo = FALSE}
require(rpart.plot)
cols <- ifelse(t1$frame$yval == 1, "gray50", "black")
prp(t1, main="Tree for Effect",
    extra=106, # display prob of survival and percent of obs
    nn=TRUE, # display node numbers
    fallen.leaves=TRUE, # put leaves on the bottom of page
    branch=.5, # change angle of branch lines
    faclen=0, # do not abbreviate factor levels
    trace=1, # print automatically calculated cex
    shadow.col="gray", # shadows under the leaves
    branch.lty=1, # draw branches using solid lines
    branch.type=5, # branch lines width = weight(frame$wt), no. of cases here
    split.cex=1.2, # make split text larger than node text
    split.prefix="is ", # put "is " before split text
    split.suffix="?", # put "?" after split text
    col=cols, border.col=cols, # cols[2] if survived
    split.box.col="lightgray", # lightgray split boxes (default is white)
    split.border.col="darkgray", # darkgray border on split boxes
    split.round=0.5) # round the split box corners a tad
```

<!-- # ```{r} -->
<!-- # require(ROCR) -->
<!-- # #plot reciever operating curve -->
<!-- # perf <- performance(preds,"tpr","fpr") -->
<!-- # ``` -->

<!-- # ```{r} -->
<!-- # # compute in-sample results -->
<!-- # caret::confusionMatrix(fitted,as.factor(acute_test$effect_f)) -->
<!-- # ``` -->
<!-- #  -->
<!-- # ```{r} -->
<!-- # # Validation set assessment #1: looking at confusion matrix -->
<!-- # prediction_for_table <- predict(acuterf, acute_test[,-6]) -->
<!-- # table(observed = acute_test[,6], predicted = prediction_for_table) -->
<!-- # ``` -->


<!-- # ```{r} -->
<!-- # # Validation set assessment #2: ROC curves and AUC -->
<!-- # # Needs to import ROCR package for ROC curve plotting: -->
<!-- # library(ROCR) -->
<!-- # # Calculate the probability of new observations belonging to each class -->
<!-- # # prediction_for_roc_curve will be a matrix with dimensions data_set_size x number_of_classes -->
<!-- # prediction_for_roc_curve <- predict(acuterf, acute_test %>% dplyr::select(-effect_f), -->
<!-- #                                     type="response") -->
<!-- # # Use pretty colours: -->
<!-- # pretty_colours <- c("#F8766D","#00BA38","#619CFF") -->
<!-- # # Specify the different classes  -->
<!-- # classes <- levels(acute_test$organism.group) -->
<!-- # # For each class -->
<!-- # for (i in 1:7) -->
<!-- # { -->
<!-- #  # Define which observations belong to class[i] -->
<!-- #  true_values <- ifelse(acute_test %>% dplyr::select(-effect_f) ==classes[i],"Y","N") -->
<!-- #  # Assess the performance of classifier for class[i] -->
<!-- #  pred <- prediction(prediction_for_roc_curve[,i],true_values) -->
<!-- #  perf <- performance(pred, "tpr", "fpr") -->
<!-- #  if (i==1) -->
<!-- #  { -->
<!-- #      plot(perf,main="ROC Curve",col=pretty_colours[i])  -->
<!-- #  } -->
<!-- #  else -->
<!-- #  { -->
<!-- #      plot(perf,main="ROC Curve",col=pretty_colours[i],add=TRUE)  -->
<!-- #  } -->
<!-- #  # Calculate the AUC and print it to screen -->
<!-- #  auc.perf <- performance(pred, measure = "auc") -->
<!-- #  print(auc.perf@y.values) -->
<!-- # } -->
<!-- # ``` -->


<!-- # ```{r} -->
<!-- # # predict() -->
<!-- # # returns out of bag predictions for training data -->
<!-- # # in the bag: every time a tree is built, it uses ~80% of the original 75% we set aside from the original dataset used to create a tree to assure random data selection -->
<!-- # # out of bag: looking at the remaining 20% of the training data to predict, when you want to know what your model does at the training location sites -->
<!-- #  -->
<!-- # # # Predict CRAM scores state-wide for all COMIDs. -->
<!-- # # notTrain_prediction <- notTrain %>% # taking all available IDs, that haven't been used in training -->
<!-- # #   na.omit() %>% # remove NAs -->
<!-- # #   mutate(effect_predicted = predict(myrf, newdata = notTrain %>% na.omit())) # using developed model (myrf), inputting predictor variables (notTrain - which contains IDs and associated  data) to predict output/dependent variable (effect_predicted a.k.a. effect). -->
<!-- #  -->
<!-- # # rePredict CRAM scores for training data. -->
<!-- # multiVar_small_train$effect_predicted <- predict(myrf) # Add column of predicted CRAM values to training dataset. -->
<!-- #  -->
<!-- # # Creates new dataset of bound rows for both ... -->
<!-- # # effect_predictions <- bind_rows(nottrain_prediction %>% -->
<!-- # #                             mutate(Set = "Non-training"), # statewide COMIDs (not used for training) -->
<!-- # #                             multiVar_smal_train %>% -->
<!-- # #                             mutate(Set = "Training")) # COMIDS from training dataset -->
<!-- # # This creates the dataset that will be plotted to create a state-wide plot of predicted CRAM scores. -->
<!-- #  -->
<!-- # # Plot the data. -->
<!-- # multiVar_small_train %>%  -->
<!-- #   mutate(effect_10 = case_when( #convert ordinal to numeric -->
<!-- #      effect_predicted == "Yes" ~ 1, -->
<!-- #      effect_predicted == "No" ~ 0 -->
<!-- #    )) %>%  -->
<!-- #   ggplot(aes(x = log10(dose.mg.L.master), y = effect_10)) + -->
<!-- #   geom_point(alpha = 0.7) +#color = "black") + -->
<!-- #   labs(x = "Density", -->
<!-- #     y = "Predicted CRAM Score") + -->
<!-- #   theme_dark() #+ -->
<!-- #   #facet_wrap(.~polymer) -->
<!-- # ``` -->
<!-- ### Predictor Selector -->
<!-- ```{r} -->
<!-- # Using caret to select the best predictors -->
<!-- # What are the parameters you want to use to run recursive feature elimination (rfe)? -->
<!-- my_ctrl <- rfeControl(functions = rfFuncs, -->
<!--                       method = "cv", -->
<!--                       verbose = FALSE, -->
<!--                       returnResamp = "all") -->

<!-- # rfe = recursive feature elimination -->
<!-- # THIS STEP TAKES FOR-EV-ER!!! -->
<!-- set.seed(22) -->
<!-- my_rfe <- rfe(y = rf_dat$effect_f, # set dependent variable -->
<!--               x = rf_dat %>% select(-effect_f), # set predictor variables -->
<!--               size = c(1:2, 4, 6, 8, 10, 13), # sets how many variables are in the overall model -->
<!--               # I have 13 total possible variables, so I've chosen increments of 3 to look at. -->
<!--               rfeControl = my_ctrl) # pull in control from above -->

<!-- # can you make your model even simpler? -->
<!-- # the following will pick a model with the smallest number of predictor variables based on the tolerance ("tol") that you specify (how much less than the best are you willing to tolerate?) -->
<!-- my_size <- pickSizeTolerance(my_rfe$results, metric = "Accuracy", tol = 1, maximize = F) -->
<!-- # higher tol (~10) gives you less variables -->
<!-- # lower tol (~1) gives you more variables - "I'd like the simplest model within 1% of the best model." -->
<!-- pickVars(my_rfe$variables, size = my_size) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # Predict scores using the above 2 variables: -->

<!-- # Create re-finalized training dataset and include all possible variables.  -->
<!-- rf_dat2 <- multiVar_small_train %>%  select(dose.mg.L.master, organism.group, effect_f)  -->

<!-- set.seed(4) # assures the data pulled is random, but sets it for the run below (makes outcome stable) -->
<!-- myrf2 <- randomForest(y = rf_dat2$effect_f, # dependent variable -->
<!--   x = rf_dat2 %>% -->
<!--     select(-effect_f), -->
<!--   importance = T,  -->
<!--   proximity = T,  -->
<!--   ntrees = 500)   -->

<!-- myrf2 # examine the results.  -->
<!-- ``` -->


<!-- ```{r} -->
<!-- importance <- as.data.frame(as.table(myrf$importance)) -->

<!-- # Nicer ggplot variable importance plot. -->
<!-- vip_plot_a <- importance %>% -->
<!--  filter(Var2 == "MeanDecreaseAccuracy") %>% -->
<!--   mutate(Var1 = factor(Var1)) %>% -->
<!--   mutate(Var1_f = fct_reorder(Var1, Freq)) %>% -->
<!--   ggplot(aes(x = Freq, y = Var1_f)) + -->
<!--   geom_point(size = 3, alpha = 0.75, color = "black") + -->
<!--   labs(x = "Importance", -->
<!--     y = "Variables") + -->
<!--   theme_bw() -->

<!-- # vip_plot_b <- importance %>% -->
<!-- #   filter(Var2 == "MeanDecreaseGini") %>% -->
<!-- #   mutate(Var1 = factor(Var1)) %>% -->
<!-- #   mutate(Var1_f = fct_reorder(Var1, Freq)) %>% -->
<!-- #   ggplot(aes(x = Freq, y = Var1_f)) + -->
<!-- #   geom_point(size = 3, alpha = 0.75, color = "black") + -->
<!-- #   labs(x = "Node Purity", -->
<!-- #     y = "Variables") + -->
<!-- #   theme_bw() -->

<!-- vip_plot <- vip_plot_a #+ vip_plot_b -->

<!-- vip_plot -->
<!-- # ggsave("cram_vip_plot.png", -->
<!-- #      path = "/Users/heilil/Desktop/R_figures", -->
<!-- #      width = 25, -->
<!-- #      height = 10, -->
<!-- #      units = "cm" -->
<!-- #    ) -->
<!-- ``` -->
GINI importance measures the average gain of purity by splits of a given variable. If the variable is useful, it tends to split mixed labeled nodes into pure single class nodes. Splitting by a permuted variables tend neither to increase nor decrease node purities. Permuting a useful variable, tend to give relatively large decrease in mean gini-gain. GINI importance is closely related to the local decision function, that random forest uses to select the best available split. Therefore, it does not take much extra time to compute. On the other hand, mean gini-gain in local splits, is not necessarily what is most useful to measure, in contrary to change of overall model performance. Gini importance is overall inferior to (permutation based) variable importance as it is relatively more biased, more unstable and tend to answer a more indirect question.


### Quantile Regression model
<!-- # ```{r} -->
<!-- # # Quantile random forest regression mode, instead of looking at the mode of trees, can compare to 10th, 50th, 90th percentiles etc. -->
<!-- #  -->
<!-- # # Need to make a new dataset taking the above results of pickVars into account. -->
<!-- # # Create finalized training dataset and include all possible variables.  -->
<!-- # qrf_dat <- multiVar_small_train  %>% -->
<!-- #   select(-effect_predicted, -ID) -->
<!-- # #   select(asci, RdCrsWs, PctAgWs, PctUrbWsRp100, PctOpWsRp100, PctOpWs, DamDensWs, RdDensWs, NABD_DensWs, PctUrbWs, PctUrbCatRp100, RdDensWsRp100, PctOpCat, PctUrbCat, RdDensCat, CBNFWs, PctOpCatRp100, PctAgWsRp100, TRIDensWs, AgKffactWs, FertWs)  -->
<!-- #  -->
<!-- #  set.seed(20) -->
<!-- #  myqrf <- quantregForest(y = qrf_dat$effect_f, # dependent variable -->
<!-- #               x = qrf_dat %>% -->
<!-- #                   select(-effect_f), -->
<!-- #               importance = T, -->
<!-- #               proximity = T, -->
<!-- #               keep.inbag=T, -->
<!-- #               ntrees = 500) -->
<!-- #  -->
<!-- # #predict(myqrf) # automatically presents 10th %tile, median, and 90th %tile -->
<!-- # ``` -->

```{r}
#predict(myqrf, what=c(0.2, 0.3, 0.999)) # to print specific quantiles
```


<!-- # ```{r} -->
<!-- # plot(myqrf) # plots the results. -->
<!-- # ``` -->
Again appears to improve after ~100 trees.

### Model Validation
#### Categorical Response


#### Continuous Response
```{r eval=FALSE, include=FALSE}
# Compare predicted vs. actual results, including by size.
# Adding lines of slope=1 and linear models to each plot.
val1 <- multiVar_small_train %>% 
  ggplot(aes(x = effect_predicted, y = effect_f)) +
  geom_point(color = "#2A3927", alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "#2A3927") +
  labs(x = "Predicted Effect",
    y = "Effect measured",
    title = "Training Data\nn=613") +
  geom_abline(intercept = 0, slope = 1) +
  theme_bw()

val1

lm1 <- lm(cram~cram_predicted, data = mydf2_train2)
summary(lm1)

val2 <- ggplot(mydf2_test2, aes(x = cram_predicted, y = cram)) +
  geom_point(color = "#3793EC", alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "#3793EC") +
  scale_x_continuous(breaks = c(0.5, 0.7, 0.9)) +
  labs(x = "CRAM predicted",
    y = "CRAM measured",
    title = "Testing Data\nn=202") +
  geom_abline(intercept = 0, slope = 1) +
  theme_bw()

val2

lm2 <- lm(cram~cram_predicted, data = mydf2_test2)
summary(lm2)

# Create the full testing + training dataset to plot together.
mydf2_test2$set <- "Testing"
mydf2_train2$set <- "Training"
full_train_test <- bind_rows(mydf2_test2, mydf2_train2) %>%
  mutate(set_f = factor(set, levels = c("Training", "Testing")))

val3 <- ggplot(full_train_test, aes(x = cram_predicted, y = cram, color = set_f)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  scale_color_manual(name = "Set", values = c("#2A3927", "#3793EC"), drop = FALSE) +
  labs(x = "CRAM predicted",
    y = "CRAM measured",
    title = "All Data\nn=815") +
  geom_abline(intercept = 0, slope = 1, color = "black") +
  facet_wrap(~PSA6) +
  theme_bw()

val3

val_fig <- (val1 + val2) /
  (val3)

val_fig + plot_annotation(
  title = 'CRAM Random Forest Results',
  subtitle = 'All modeling performed using StreamCAT datasets.',
  caption = 'Linear models are colored according to dataset. Lines of slope = 1 are denoted in black.'
)

# Save figure.
# ggsave("cram_rfmodel_validation.png",
#      path = "/Users/heilil/Desktop/R_figures",
#      width = 35,
#      height = 25,
#      units = "cm"
#    )

lm3 <- lm(cram~cram_predicted, 
  data = full_train_test %>%
    filter(PSA6 == "Central_Valley") %>%
    filter(set_f == "Training"))

lm4 <- lm(cram~cram_predicted, 
  data = full_train_test %>%
    filter(PSA6 == "Central_Valley") %>%
    filter(set_f == "Testing"))

lm5 <- lm(cram~cram_predicted, 
  data = full_train_test %>%
    filter(PSA6 == "Chaparral") %>%
    filter(set_f == "Training"))

lm6 <- lm(cram~cram_predicted, 
  data = full_train_test %>%
    filter(PSA6 == "Chaparral") %>%
    filter(set_f == "Testing"))

lm7 <- lm(cram~cram_predicted, 
  data = full_train_test %>%
    filter(PSA6 == "Deserts_Modoc") %>%
    filter(set_f == "Training"))

lm8 <- lm(cram~cram_predicted, 
  data = full_train_test %>%
    filter(PSA6 == "Deserts_Modoc") %>%
    filter(set_f == "Testing"))

lm9 <- lm(cram~cram_predicted, 
  data = full_train_test %>%
    filter(PSA6 == "North_Coast") %>%
    filter(set_f == "Training"))

lm10 <- lm(cram~cram_predicted, 
  data = full_train_test %>%
    filter(PSA6 == "North_Coast") %>%
    filter(set_f == "Testing"))

lm11 <- lm(cram~cram_predicted, 
  data = full_train_test %>%
    filter(PSA6 == "Sierra") %>%
    filter(set_f == "Training"))

lm12 <- lm(cram~cram_predicted, 
  data = full_train_test %>%
    filter(PSA6 == "Sierra") %>%
    filter(set_f == "Testing"))

lm13 <- lm(cram~cram_predicted, 
  data = full_train_test %>%
    filter(PSA6 == "South_Coast") %>%
    filter(set_f == "Training"))

lm14 <- lm(cram~cram_predicted, 
  data = full_train_test %>%
    filter(PSA6 == "South_Coast") %>%
    filter(set_f == "Testing"))

# Chose not to compute confusion matrix / accuracy score since this is more applicable to categorical ouputs from random forest models -
# Instead, calculated Root Mean Squared Error (RMSE) of both training and test datasets.
# If test RMSE values are much greater than training, then possible the model has been over fit.

predtest <- predict(myrf2, mydf2_test2)
rmse(mydf2_test2$cram,predtest)
# 9.03

predtrain <- predict(myrf2, mydf2_train2)
rmse(mydf2_train2$cram,predtrain)
# 4.20

# Double checking using the original random forest dataset (rf_dat) with all 35 possible variables included to see where the error in number of predictors starts to increase dramatically (to help double check our decision to include 25 parameters).
dc <- rfcv(rf_dat %>%
    select(-cram), 
  rf_dat$cram,
  step = 0.7, # default is 0.5
  scale="log")

dc$error.cv
#34        24        17        12         8         6         4         3         2 
# 96.28062  97.29366  99.08743 102.93752 115.52580 120.63049 122.09563 123.74425 139.30181 
# 1 
# 206.58921 

# Appears between 34 and 24 variables, there is an insignificant increase in error.
# However, this model is much larger than the CSCI (20) and ASCI (10) models, so we may decide to trim this down in the future.

```

#3-D plots
```{r}
#remotes::install_github("tylermorganwall/rayshader")
#require(rayshader)
require(tidyverse)
require(ggsci)
require(ggdark)

#scatterplot with size, dose and polymer
acute3D <- acute %>% 
  #filter(effect_f == "Yes") %>% 
  ggplot(aes(x = size.length.um.used.for.conversions, y = dose.mg.L.master, color = effect.score)) + #, shape = polymer)) +
  geom_point(alpha = 0.6) +
  geom_smooth() +
  scale_y_continuous("Dose (mg/L)", limits=c(-4,5)) +
  scale_color_viridis_c(option = "A") +
  ggtitle("Toxicity Probability by Length, Dose (mg/L) and Shape") +
  labs(caption = "Acute Aquatic Organism") +
  dark_theme_classic() +
  facet_wrap(shape~.)

acute3D

#3D plot
#plot_gg(acute3D, multicore = TRUE, width = 5, height = 5, scale =250) #3D plot
```

```{r eval=FALSE, include=FALSE}
#3d scatterplot
require(plotly)
# size = acute$size.length.um.used.for.conversions
# dose = acute$dose.mg.L.master
# effect = acute$effect.score
# volume = acute$particle.volume.um3

plot_ly(acute, 
        x = ~size.length.um.used.for.conversions, 
        y = ~dose.mg.L.master, 
        z = ~particle.volume.um3, 
        type = "scatter3d", 
        mode = "markers", 
        color = ~effect_f)
```
```{r}
#scatterplot with size, dose and polymer
mass <-multiVar %>% 
  filter(!effect_f == "NA") %>% 
  filter(effect.metric == c("HONEC", "LOEC", "NOEC")) %>% 
  filter(!acute.chronic_f == "Unavailable") %>% 
  filter(bio.org == "organism") %>% 
  filter(!environment == "Terrestrial") %>% 
  filter(lvl1_f == "Fitness") %>% 
  ggplot(aes(x = size.length.um.used.for.conversions, y = dose.mg.L.master, color = effect_f)) + 
  geom_point(alpha = 0.5) +
  geom_smooth() +
  scale_y_continuous(limits = c(0, 300))+
 # scale_y_log10("Dose (mg/L)",limits = c(1e-4, 1e7))+
  scale_x_log10("Length (um)", limits  = c(1, 1e3)) +
  #coord_trans(x = "log10") +
  #scale_x_continuous("Length (um)", breaks = scales::trans_breaks("log10", function(x) 10^x, n = 5),
                   #      labels = trans_format("log10", scales::math_format(10^.x))) +
  scale_colour_locuszoom() +
  ggtitle("Toxicity Probability by Length and Dose (mg/L)") +
  labs(caption = "Aquatic Organisms, HONEC/LOEC/NOEC") +
  dark_theme_classic()# + 
  #facet_wrap(acute.chronic_f ~.)
mass
```
```{r}
particles <- multiVar %>% 
  filter(!effect_f == "NA") %>% 
  filter(effect.metric == c("HONEC", "LOEC", "NOEC")) %>% 
  filter(!acute.chronic_f == "Unavailable") %>% 
  filter(bio.org == "organism") %>% 
  filter(!environment == "Terrestrial") %>% 
  filter(lvl1_f == "Fitness") %>% 
  ggplot(aes(x = size.length.um.used.for.conversions, y = dose.particles.mL.master, color = effect_f)) + 
  geom_point(alpha = 0.5) +
  geom_smooth() +
  scale_y_log10("Dose (particles/mL)",limits = c(1e-4, 1e7))+
  scale_x_log10("Length (um)", limits  = c(1, 1e4)) +
  #coord_trans(x = "log10") +
  #scale_x_continuous("Length (um)", breaks = scales::trans_breaks("log10", function(x) 10^x, n = 5),
                   #      labels = trans_format("log10", scales::math_format(10^.x))) +
  scale_colour_locuszoom() +
  ggtitle("Toxicity Probability by Length and Dose (particles/mL)") +
  labs(caption = "Aquatic Organisms, HONEC/LOEC/NOEC") +
  dark_theme_classic() #+ 
  #facet_wrap(acute.chronic_f ~.)
particles
```
```{r}
volume <- aoc_z %>% 
  filter(!effect_f == "NA") %>% 
  filter(effect.metric == c("HONEC", "LOEC", "NOEC")) %>% 
  filter(!acute.chronic_f == "Unavailable") %>% 
  filter(bio.org == "organism") %>% 
  filter(!environment == "Terrestrial") %>% 
  filter(lvl1_f == "Fitness") %>% 
  ggplot(aes(x = size.length.um.used.for.conversions, y = dose.um3.mL.master, color = effect_f)) + 
  geom_point(alpha = 0.5) +
  geom_smooth() +
  scale_y_log10("Dose (um3/mL)",limits = c(1e+1, 1e7))+
  scale_x_log10("Length (um3)", limits  = c(1e-1, 100)) +
  #coord_trans(x = "log10") +
  #scale_x_continuous("Length (um)", breaks = scales::trans_breaks("log10", function(x) 10^x, n = 5),
                   #      labels = trans_format("log10", scales::math_format(10^.x))) +
  scale_colour_locuszoom() +
  ggtitle("Toxicity Probability by Length and Dose (um3/mL)") +
  labs(caption = "Aquatic Organisms, HONEC/LOEC/NOEC") +
  dark_theme_classic() #+ 
  #facet_wrap(acute.chronic_f ~.)
volume
```

```{r}
require(gridExtra)
grid.arrange(particles, mass, volume, nrow = 3)
```

```{r}
#scatterplot with size, dose and polymer
taxa_mass <-aoc_z %>% 
  filter(!effect_f == "NA") %>% 
 # filter(effect.metric == c("HONEC", "LOEC", "NOEC")) %>% 
  filter(!acute.chronic_f == "Unavailable") %>% 
  filter(bio.org == "organism") %>% 
  filter(!environment == "Terrestrial") %>% 
  filter(lvl1_f == "Fitness") %>% 
  ggplot(aes(x = size.length.um.used.for.conversions, y = dose.mg.L.master.AF.noec, color = effect_f)) + 
  geom_point(alpha = 0.5) +
  geom_smooth() +
  scale_y_continuous(limits = c(0, 300))+
 # scale_y_log10("Dose (mg/L)",limits = c(1e-4, 1e7))+
  scale_x_log10("Length (um)", limits  = c(1, 1e3)) +
  #coord_trans(x = "log10") +
  #scale_x_continuous("Length (um)", breaks = scales::trans_breaks("log10", function(x) 10^x, n = 5),
                   #      labels = trans_format("log10", scales::math_format(10^.x))) +
  scale_colour_locuszoom() +
  ggtitle("Toxicity Probability by Length and Dose (mg/L)") +
  labs(caption = "Aquatic Organisms, HONEC/LOEC/NOEC") +
  dark_theme_classic() + 
  facet_wrap(organism.group ~.)
taxa_mass
```

```{r}
#Logistic Regression for acute fitness 
m1_crust <-aoc_z %>% 
  filter(!effect_f == "NA") %>% 
 # filter(effect.metric == c("HONEC", "LOEC", "NOEC")) %>% 
# filter(organism.group == "Crustacea") %>% 
  filter(!acute.chronic_f == "Unavailable") %>% 
  filter(bio.org == "organism") %>% 
  filter(!environment == "Terrestrial") %>% 
  filter(lvl1_f == "Fitness") %>% 
  filter(!size_f == "Not Reported") %>% 
  mutate(logdose.mg.L.master = log10(dose.mg.L.master))#%>% 
 # filter(acute.chronic_f == "Acute")

m1_crust_model <- glm(effect_10 ~ (size.length.um.used.for.conversions + log10(dose.mg.L.master) +
                                     log10(dose.particles.mL.master) + organism.group) ^ 2, #exponent gives all 2-way interactions
    data = m1_crust, na.action = "na.exclude", family = "binomial")

summary(m1_crust_model)
```
```{r}
m1_crust_model$coef
```

## Plot binomial
```{r}
m1_crust_simple <- m1_crust %>% 
  dplyr::select(c(effect_10, logdose.mg.L.master, size.length.um.used.for.conversions)) %>% 
  drop_na
#simple single parameter
m1_crust_model_dose <- glm(effect_10 ~ logdose.mg.L.master * size.length.um.used.for.conversions, #exponent gives all 2-way interactions
    data = m1_crust_simple, na.action = "na.exclude", family = "binomial")

summary(m1_crust_model_dose)
```


```{r eval=FALSE, include=FALSE}
range(m1_crust_simple$logdose.mg.L.master, na.rm = TRUE)
range(m1_crust_simple$size.length.um.used.for.conversions, na.rm = TRUE)
#[1] -11.638272   8.171552
# generate sequence of values between range in increments of 100 to give smooth appearance to model
xdose <- seq(-11.638272 ,8.171552, 0.1)
colorsize <- seq(3.45e-02, 5.00e+03, 1)
ydose <- predict(m1_crust_model_dose, 
                 list(logdose.mg.L.master = xdose,
                      size.length.um.used.for.conversions = colorsize), 
                 type = "response")
#plot
# plot(m1_crust_simple$logdose.mg.L.master, m1_crust_simple$effect_10, pch = 16, xlab = "log dose (mg/L)", ylab = "effect")
# lines(xdose, ydose)

ggplot(m1_crust_simple, aes(x = logdose.mg.L.master, y = effect_10))+
  geom_point() +
  stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) +
  labs(title = "Crustacea Fitness Binomial",
       caption = "1,348 observations, dose p=6e-9") +
  theme_minimal()
```

```{r}
#alternative plot with ggpredict
#devtools::install_github("cardiomoon/ggiraphExtra")
require(ggiraphExtra)

ggPredict(m1_crust_model_dose,interactive=TRUE,colorn=100,jitter=FALSE)
```
What happens if we log-transfrom size?


What about volume?
```{r}
m1_crust_simple_volume <- m1_crust %>% 
  mutate(logdose.um3.mL.master = log10(dose.um3.mL.master)) %>% 
  dplyr::select(c(effect_10, logdose.um3.mL.master, size.length.um.used.for.conversions)) %>% 
  drop_na
#simple single parameter
m1_crust_model_volume<- glm(effect_10 ~ logdose.um3.mL.master * size.length.um.used.for.conversions, #exponent gives all 2-way interactions
    data = m1_crust_simple_volume, na.action = "na.exclude", family = "binomial")

summary(m1_crust_model_volume)
```
```{r}
#plot logistic volume
require(ggiraphExtra)
ggPredict(m1_crust_model_volume,interactive=TRUE,colorn=100,jitter=FALSE)
```

```{r}
#scatterplot with size, dose and polymer
mass_probability_crustacea <-aoc_z %>% 
  filter(!effect_f == "NA") %>% 
 # filter(effect.metric == c("HONEC", "LOEC", "NOEC")) %>% 
  filter(organism.group == "Crustacea") %>% 
  filter(!acute.chronic_f == "Unavailable") %>% 
  filter(bio.org == "organism") %>% 
  filter(!environment == "Terrestrial") %>% 
  filter(lvl1_f == "Fitness") %>% 
  filter(!size_f == "Not Reported") %>% 
  droplevels() %>% 
  ggplot(aes(x = dose.mg.L.master.AF.noec, y = effect_10, color = effect_f)) + 
  geom_point(alpha = 0.5) +
  geom_smooth() +
  scale_y_continuous(limits = c(0, 1))+
 scale_x_log10("Dose (mg/L)",limits = c(1e-4, 1e7))+
  #scale_x_log10("Length (um)", limits  = c(1, 1e3)) +
  #coord_trans(x = "log10") +
  #scale_x_continuous("Length (um)", breaks = scales::trans_breaks("log10", function(x) 10^x, n = 5),
                   #      labels = trans_format("log10", scales::math_format(10^.x))) +
  scale_colour_locuszoom() +
  ggtitle("Toxicity Probability by Length and Dose (mg/L)") +
  labs(caption = "Aquatic Organisms, HONEC/LOEC/NOEC") +
  dark_theme_classic() + 
  facet_wrap(size_f ~.)
mass_probability_crustacea
```

```{r eval=FALSE, include=FALSE}
#3D density plot
acute_density <- acute %>% 
  ggplot(aes(x = size.length.um.used.for.conversions, y = dose.mg.L.master)) +
  stat_density_2d(aes(fill = stat(effect.score)),
                       geom = "polygon",
                       n = 100, bins = 10, contour = TRUE) +
  facet_wrap(shape~.) +
  scale_fill_viridis_c(option = "A")

plot_gg(acute_density, multicore = TRUE, width = 5, height = 5, scale =250) #3D plot
```

```{r eval=FALSE, include=FALSE}
#heatmap
#meltAcute = reshape2::melt(acute, id.vars = "size.length.um.used.for.conversions")

#meltAcute$age = as.numeric(meltAcute$variable)

acutegg = acute %>% 
  #scale() or normalize() %>% 
  ggplot() +
  geom_raster(aes(x = size.length.um.used.for.conversions,y = dose.mg.L.master, fill = effect_f)) +
  scale_x_continuous("Size (um)",expand=c(0,0)) + #,breaks = seq(1900,2010,10)) +
  scale_y_continuous("Dose (mg/L)", expand=c(0,0)) + #,breaks=seq(0,100,10),limits=c(0,100)) +
  scale_fill_viridis_d("Death\nProbability\nPer Year") +#,trans = "log10",breaks=c(1,0.1,0.01,0.001,0.0001), labels = c("1","1/10","1/100","1/1000","1/10000")) +
  ggtitle("Acute Toxicity Probability by Size and Dose") +
  labs(caption = "Acute Aquatic Organism")

acutegg
#plot_gg(deathgg, multicore=TRUE,height=5,width=6,scale=500)
```



# Works Cited
