---
title: "Win_mod_interpretation"
author: "Win Cowger"
date: "4/5/2021"
output:
  html_document:
    code_folding: hide
    theme: journal
    toc: yes
    toc_float: yes
    toc_depth: 6
    number_sections: true
    includes:
      after_body: footer.html
  word_document:
    toc: yes
---

```{r global-options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      warning=FALSE, message=FALSE,time_it = TRUE) #report time to knit for all chunks

#knit time reporter
all_times <- list()  # store the time for all chunks in a list
knitr::knit_hooks$set(time_it = local({
  now <- NULL
  function(before, options) {
    if (before) {
      now <<- Sys.time()
    } else {
      res <- difftime(Sys.time(), now)
      all_times[[options$label]] <<- res
    }
  }
}))
```


```{r message=FALSE, warning=FALSE, include=FALSE}
# load required packages
library(rsample)
library(dplyr)
library(DALEX)
library(tidyverse)
library(tigerstats)
library(caret)
library(skimr)
library(ggeffects)
library(knitr)
library(doParallel)
library(modelplotr)
library(gridExtra)
library(interpret) #explainable boosting machine
library(infotheo)
library(tidyinftheo)
library(sidekicks)
```

```{r include=FALSE}
require(readr)
#load aoc_z into dataframe. This file is generated from RDA_Maker.R
#source("Tox Data/RDA_Maker.R")
aoc_z <- readRDS(file = "Tox Data/aoc_z.Rda")

```

#Look at the data highlight potential variable issues for modeling.

```{r include = FALSE}
aoc_z_characters <- aoc_z %>%
  mutate_if(is.factor, as.character)

#tells us if any of the variables have more than 50% of the data missing. 
skimmed_aoc_flag_missing <- skim(aoc_z_characters) %>%
  filter(n_missing > 0.5*nrow(aoc_z)) %>%
  pull(skim_variable)

#tells us if any of the variables have more variable groups than 10% of the total rows 
skimmed_aoc_flag_deep <- skim(aoc_z_characters) %>%
  filter(character.n_unique > 0.1*nrow(aoc_z)) %>%
  pull(skim_variable)

#variables that only have one group (not useful)
skimmed_aoc_flag_shallow <- skim(aoc_z_characters) %>%
  filter(character.n_unique == 1) %>%
  pull(skim_variable)

#tells us which variables have character groups that have fewer than 3 observations.
skimmed_aoc_inadequate_n_char <- skim(aoc_z_characters) %>%
  filter(character.min < 3) %>%
  pull(skim_variable)
```


# Data Selection

**Important**: this dataset has ONLY been filtered for 'red' criteria for technical quality.

*Suggested variables*: organism.group, acute.chronic_f, lvl2_f, bio.org, dose.mg.L.master, polymer,  particle.volume.um3, particle.surface.area.um2, particle.surface.area.um2.mg, mass.per.particle.mg, shape, max.size.ingest.mm

The above variables should have the lowest possible colinearity while maintaining the critical predictor variables. While surface area and volume are related, they are actually non-linear. Having a single dose descriptor (mass) alongside important particle characteristics (surface area, volume, mass per particle) allows for all ecologically relevant metrics to be accounted for while avoiding colinearity. Leading hypotheses for ecologically relevant metrics include food dilution (volume), oxidative stress (surface area), and inflammation (length:width ratio - covered by shape). Ideally, species or even genus would be included, but there are too many levels to split data into test/train and retain all levels in both. This is really a shame, because some species are FAR more sensitive than others. Lacking this, organism group accounts ROUGHLY for this variabilty.

```{r}
df <- aoc_z_characters %>% #use factors
  filter(tech.tier.zero == "Pass") %>% #gives studies that pass technical quality criteria
  #filter(risk.tier.zero == "Pass") %>%  #only studies applicable for risk assessment. VERY RESTRICTIVE 
  dplyr::select(c(
    ### Organism characteristics ###
    organism.group, # very general organism group (e.g. fish, crustacea, etc.) - 13 levels
    #species_f, #specific species (113 levels)
    #genus, # 96 levels
    max.size.ingest.mm,
    acute.chronic_f, #binary classification - species specific (makes HUGE difference)
    lvl1_f, # general endpoint (e.g. fitness, behavior) (9 levels)
    lvl2_f, #specific endpoint (e.g. mortality, growth)(45 levels)
    #environment,# freshwater, marine or terrestrial (should make little difference)
    #life.stage, #4 levels, 100% complete (moderately important)
    bio.org, # (organismal, tissue, population, etcl) 5 levels, 100% complete (highly important)
    exposure.route, #water, food, sediment 
    ## dose metrics ##
    dose.um3.mL.master, #volume/volume dose
    dose.mg.L.master, #mass/volume dose
    dose.particles.mL.master, #particle/volume dose
    dose.surface.area.um2.mL.master, #area/volume dose
    dose.specific.surface.area.um2.mg.mL, # specific surface area/volume dose
    ## Particle characteristics ##
    polymer, #14 levels
    shape, #4 levels
    #size.length.um.used.for.conversions, #continuous, 99.5% complete
    particle.volume.um3, #continuous, 89% complete
    #density.mg.um.3, #continous, 97% complete
    particle.surface.area.um2, #continuous surface area
    particle.surface.area.um2.mg, #specific surface area
    mass.per.particle.mg, # particle mass
    ## Response variable
    effect, # BINARY (y/n) 100% complete
   # effect.metric #Honec, noec, loec, etc. NOTE: NA is an effect, but somewhere along the curve (unassigned magnitude)
                  )) %>% 
  # mutate(effect.metric = as.character(effect.metric)) %>% 
  # mutate(effect.metric = (case_when(
  #   effect.metric == "NONEC" ~ "NOEC",
  #   effect.metric == "HONEC" ~ "NOEC",
  #   effect.metric == "LOEC" ~ "LOEC",
  #   effect.metric == "LC50" ~ "LC50",
  #   effect.metric == "EC50" ~"EC50",
  #   effect.metric == "EC10" ~ "EC10"
  # ))) %>% 
  #mutate(effect.metric = replace_na(effect.metric,"not_available")) %>% 
  #filter(effect.metric != "not_available") %>% 
  filter(exposure.route == "water") %>% 
  # mutate(effect.metric = as.factor(effect.metric)) %>% 
  drop_na() %>%  #drop missing
#mutate_if(~is.numeric(.) && (.) > 0, log10) %>% 
  mutate(effect_10 = case_when( #convert ordinal to numeric
      effect == "Y" ~ 1,
      effect == "N" ~ 0
    )) %>%
  mutate(effect_10 = factor(effect_10)) %>% 
  mutate_if(is.character, as.factor) %>% 
  dplyr::select(-c(effect, 
                   exposure.route#,
                   #effect.metric
                   )) %>%
  mutate(sa_vol_ratio = dose.surface.area.um2.mL.master/dose.um3.mL.master)

#ensure completeness
skim(df)
```

#Look at mututal information between variables

```{r include=F}
library(tidyverse)
library(rcompanion)


# Calculate a pairwise association between all variables in a data-frame. In particular nominal vs nominal with Chi-square, numeric vs numeric with Pearson correlation, and nominal vs numeric with ANOVA.
# Adopted from https://stackoverflow.com/a/52557631/590437
mixed_assoc = function(df, cor_method="spearman", adjust_cramersv_bias=TRUE){
    df_comb = expand.grid(names(df), names(df),  stringsAsFactors = F) %>% set_names("X1", "X2")

    is_nominal = function(x) class(x) %in% c("factor", "character")
    # https://community.rstudio.com/t/why-is-purr-is-numeric-deprecated/3559
    # https://github.com/r-lib/rlang/issues/781
    is_numeric <- function(x) { is.integer(x) || is_double(x)}

    f = function(xName,yName) {
        x =  pull(df, xName)
        y =  pull(df, yName)

        result = if(is_nominal(x) && is_nominal(y)){
            # use bias corrected cramersV as described in https://rdrr.io/cran/rcompanion/man/cramerV.html
            cv = cramerV(as.character(x), as.character(y), bias.correct = adjust_cramersv_bias)
            data.frame(xName, yName, assoc=cv, type="cramersV")

        }else if(is_numeric(x) && is_numeric(y)){
            correlation = cor(x, y, method=cor_method, use="complete.obs")
            data.frame(xName, yName, assoc=correlation, type="correlation")

        }else if(is_numeric(x) && is_nominal(y)){
            # from https://stats.stackexchange.com/questions/119835/correlation-between-a-nominal-iv-and-a-continuous-dv-variable/124618#124618
            r_squared = summary(lm(x ~ y))$r.squared
            data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")

        }else if(is_nominal(x) && is_numeric(y)){
            r_squared = summary(lm(y ~x))$r.squared
            data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")

        }else {
            warning(paste("unmatched column type combination: ", class(x), class(y)))
        }

        # finally add complete obs number and ratio to table
        result %>% mutate(complete_obs_pairs=sum(!is.na(x) & !is.na(y)), complete_obs_ratio=complete_obs_pairs/length(x)) %>% rename(x=xName, y=yName)
    }

    # apply function to each variable combination
    map2_df(df_comb$X1, df_comb$X2, f)
}


#Mutual information
discretized <- discretize(df)
matrix <- discretized %>%
  mutinformation() 
dfmatrix <- as.data.frame(as.table(matrix))

#Correlation matrix
mixedassoc <- mixed_assoc(df)


#Correlation matrix
#aoc_no_logical <- aoc_z %>%
#  select(-c(skimmed_aoc_flag_shallow, 
#            skimmed_aoc_flag_deep, 
#            skimmed_aoc_flag_missing, 
#            skimmed_aoc_inadequate_n_char)) %>%
#  select_if(function(col) !is.logical(col)) %>%
#  mutate_if(is.factor, as.character) 

#cor_matrix <- cor2(aoc_no_logical)

```

#Stepwise regression.

```{r}
library(MASS)
training.samples <- df$effect_10 %>% 
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- df[training.samples, ]
test.data <- df[-training.samples, ]

# Fit the model
model <- glm(effect_10 ~., data = train.data, family = binomial) %>%
  stepAIC(trace = FALSE)
# Summarize the final selected model
summary(model)
# Make predictions
probabilities <- model %>% predict(test.data, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "1", "0")
# Model accuracy
mean(predicted.classes==test.data$effect_10)

library(mgcv)
library(car)
library(broom)

meanmodel <- gam(effect_10 ~ 1, data = train.data, family = binomial)
AIC(meanmodel)

nodosemodel <- gam(effect_10 ~  organism.group + bio.org + lvl1_f, data = train.data, family = binomial)
AIC(nodosemodel)

dosemodel <- glm(effect_10 ~ dose.um3.mL.master + dose.surface.area.um2.mL.master + dose.particles.mL.master + dose.mg.L.master + sa_vol_ratio, data = train.data, family = binomial)
AIC(dosemodel)
summary(dosemodel)

#looking into some of the model assumptions 
#http://www.sthda.com/english/articles/36-classification-methods-essentials/148-logistic-regression-assumptions-and-diagnostics-in-r/
probabilities <- predict(dosemodel, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "1", "0")
mydata <- train.data %>%
  dplyr::select_if(is.numeric) 
predictors <- colnames(mydata)
# Bind the logit and tidying the data for plot
#linearity
mydata <- mydata %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)

ggplot(mydata, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")
#not super linear, perhaps some outliers.

#Influential values 
plot(dosemodel, which = 4, id.n = 10)
model.data <- augment(dosemodel) %>% 
  mutate(index = 1:n()) 

model.data %>% top_n(10, .cooksd)

ggplot(model.data, aes(index, .std.resid)) + 
  geom_point(aes(color = effect_10), alpha = .5) +
  theme_bw()
#No outliers that we can remove. Need to be above 3 std.residual abs

#Multicollinearity, VIF above 5 or 10 is bad. Too much colinearity here. 
car::vif(dosemodel)

dosemodel1 <- glm(effect_10 ~ dose.um3.mL.master + dose.particles.mL.master + dose.mg.L.master + dose.surface.area.um2.mL.master + sa_vol_ratio, data = train.data, family = binomial)
AIC(dosemodel1)
summary(dosemodel1)
car::vif(dosemodel1)


full.model <- glm(effect_10 ~., data = train.data, family = binomial)
coef(full.model)
summary(full.model)
AIC(full.model)

full.model.smooth <- gam(effect_10 ~ organism.group + s(dose.um3.mL.master) + bio.org + s(dose.surface.area.um2.mL.master) + lvl1_f, data = train.data, family = binomial)
AIC(full.model.smooth)

full.model.smooth.by <- gam(effect_10 ~ s(dose.um3.mL.master, by = organism.group) + bio.org + s(dose.surface.area.um2.mL.master, by = organism.group) + lvl1_f, data = train.data, family = binomial)
AIC(full.model.smooth.by)

plot(full.model.smooth.by)
plot(full.model.smooth)

step.model <- full.model %>% stepAIC(direction = "both", trace = F)
coef(step.model)

# Make predictions
#http://www.sthda.com/english/articles/36-classification-methods-essentials/148-logistic-regression-assumptions-and-diagnostics-in-r/
probabilities <- full.model %>% predict(test.data, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "1", "0")
# Prediction accuracy
observed.classes <- test.data$effect_10
mean(predicted.classes == observed.classes)

probabilities <- predict(step.model, test.data, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "1", "0")
# Prediction accuracy
observed.classes <- test.data$effect_10
mean(predicted.classes == observed.classes)




library(tidyr)

```
#Plot model results for every species-endpoint combo observed and all concentrations and types observed.
```{r}
#Should be able to make this a function somehow. 


#rea in models
readRDS("ML Models/FinalModels/final_rf_model.rds")

df_expand = df 

org_endpoint_comb <- df %>%
  dplyr::select(organism.group, bio.org, lvl1_f) %>%
  distinct() 

df_expand <- org_endpoint_comb %>%
  expand_grid(dose.um3.mL.master = seq(min(df$dose.um3.mL.master), max(df$dose.um3.mL.master), length.out = 100), dose.surface.area.um2.mL.master = seq(min(df$dose.surface.area.um2.mL.master), max(df$dose.surface.area.um2.mL.master), length.out = 100))

#Ran the code below too to see if the glm methods were the same. They are returning the same response. 
df_expand$prob <- predict(full.model, df_expand, type = "response")

for(row in 1:nrow(org_endpoint_comb)){
  slice <- org_endpoint_comb[row,]
  doses <- inner_join(df_expand, slice) %>%
    dplyr::select(-organism.group, -bio.org, -lvl1_f) %>%
    pivot_longer(cols = -prob, names_to = "type", values_to = "values")
 plot <- ggplot(doses) + geom_point(aes(x = values, y = prob)) + facet_wrap(type~., scales = "free") + labs(title = paste0(as.character(unlist(slice)), collapse = "_"))
  ggsave(plot = plot, filename = paste0(paste0(as.character(unlist(slice)), collapse = "_"), ".png"), path = "Tox Data/figures")
  
}  
```
#May want to reconsider the above approach. I originally set it up to only opperate within concentration domains which were observed for each enpoint organism combination. That might be more appropriate, or it may be ok to extrapolate to other organisms using informaiton from other organisms.

#Test Martin's Model
This really is just an accuracy test, we can run his 0's and 1's through the model and see if they are correct. If his data is formatted to demonstrate the entire model adn coefficients, we could also try to test for a difference between the model fits. I think the observations will likely be powerful enough. 




#Species Sensitivity Distribution

```{r}

df_expand_sensitivity_distribution <- df_expand %>%
  filter(prob > 0.5) %>%
  group_by(organism.group, bio.org, lvl1_f) %>% #Can add other factors here if we want to flesh this out for each organism.
  summarise(dose.um3.mL.master = min(dose.um3.mL.master), dose.surface.area.um2.mL.master = min(dose.surface.area.um2.mL.master)) %>%
  ungroup()

ggplot(df_expand_sensitivity_distribution) + stat_ecdf(aes(x = dose.surface.area.um2.mL.master))

ggplot(df_expand_sensitivity_distribution) + stat_ecdf(aes(x = dose.um3.mL.master))


```


```{r}
# create train, validation, and test splits
# Create calibration and validation splits with tidymodels initial_split() function.
set.seed(4)
df_split <- df %>%
  initial_split(prop = 0.75)
# default is 3/4ths split (but 75% training, 25% testing).

# Create a training data set with the training() function
# Pulls from training and testing sets created by initial_split()
train <- training(df_split)
test <- testing(df_split)

# variable names for response & features
y <- "effect_10"
x <- setdiff(names(df), y) 

## subset for ebm syntax
train_x <- as.data.frame(train %>% dplyr::select(-effect_10))
train_y <- as.numeric(as.vector(train$effect_10))

test_x <- as.data.frame(test %>% dplyr::select(-effect_10))
test_y <- as.numeric(as.vector(test$effect_10))
```

```{r All Model Training, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# Examine the environment to be sure # of observations looks like the 75/25 split. 3199:1066.

####build explainable boosting machine model #####
ebm <- ebm_classify(train_x, #features
                    train_y, #target,
                            ## hyper-parameters, currently in default state:
                            max_bins = 255,
                            outer_bags = 16,
                            inner_bags = 0,
                            learning_rate = 0.01,
                            validation_size = 0.15,
                            early_stopping_rounds = 50,
                            early_stopping_tolerance = 1e-4,
                            max_rounds = 5000,
                            max_leaves = 3,
                            min_samples_leaf = 2,
                            random_state = 42)







#####build additional models using caret package and expain using DALEX package #####
classif_rf <- train(effect_10~., data = train, method = "rf", ntree = 100, tuneLength = 1)

classif_glm <- train(effect_10~., data = train, method = "glm", family = "binomial")

classif_svm <- train(effect_10~., data = train, method = "svmRadial", prob.model = TRUE, tuneLength = 1)#,
                    # preProcess = c("pca","scale","center"))

classif_decTree <- train(effect_10~., data = train, method = "C5.0", preProcess = c("scale","center")) 

classif_nnet <- train(effect_10~., data = train, method = "nnet", preProcess=c("scale","center"))

classif_xgbTree <- train(effect_10~., data = train, method = "xgbTree")


##### build explainers ####
# extract test data#
yTest <- as.numeric(as.character(test$effect_10))

#explainer_classif_ebm <- DALEX::explain(classif_ebm,
 #                                      label = "EBM",
  #                                     data = subset(test, select = -c(effect_10)), 
   #                                    y = yTest)


explainer_classif_rf <- DALEX::explain(classif_rf,
                                       label = "rf",
                                       data = test_x, 
                                       y = yTest)
                                       
explainer_classif_glm <- DALEX::explain(classif_glm, label = "glm", 
                                        data = test_x, y = yTest)
                                       
explainer_classif_svm <- DALEX::explain(classif_svm,  label = "svm", 
                                        data = test_x, y = yTest)

explainer_classif_decTree <- DALEX::explain(classif_rf, label = "Decision Tree",
                                       data = test_x, 
                                       y = yTest)
                                       
explainer_classif_nnet <- DALEX::explain(classif_glm, label = "Neural Net", 
                                        data = test_x, y = yTest)
                                       
explainer_classif_xgbTree <- DALEX::explain(classif_svm,  label = "eXtreme Gradient Boosting Trees", 
                                        data = test_x, y = yTest)
```
# Modelling
## Model Performance
```{r model performance, include=FALSE}
### EBM model ###
proba_test <- ebm_predict_proba(ebm, test_x) #keeps giving NA's for some reason...

ebm_show(ebm, name = "bio.org")

#### Caret models ####
mp_classif_rf <- model_performance(explainer_classif_rf)
mp_classif_glm <- model_performance(explainer_classif_glm)
mp_classif_svm <- model_performance(explainer_classif_svm)
mp_classif_decTree <- model_performance(explainer_classif_decTree)
mp_classif_nnet <- model_performance(explainer_classif_nnet)
mp_classif_xgbTree <- model_performance(explainer_classif_xgbTree)
```

### Histogram of Residuals
```{r}
plot(mp_classif_rf, mp_classif_glm, geom ="histogram")
```
```{r}
plot(mp_classif_svm, mp_classif_decTree, geom ="histogram")
```

```{r}
plot(mp_classif_nnet, mp_classif_xgbTree, geom ="histogram")
```
### Precision Recall Curves
```{r}
plot(mp_classif_rf, mp_classif_glm, mp_classif_svm, mp_classif_decTree, mp_classif_nnet, mp_classif_xgbTree, geom ="prc")
```


### Reverse Cumulative Distribution of Residuals
```{r}
# compare residuals plots
resid_dist <- plot(mp_classif_rf, mp_classif_glm, mp_classif_svm, mp_classif_decTree, mp_classif_nnet,
                   mp_classif_xgbTree) + #, mp_classif_logicBag) +
  theme_minimal() +
        theme(legend.position = 'bottom',
              plot.title = element_text(hjust = 0.5)) + 
        labs(y = '')
resid_dist
```
#### Residuals
```{r}
resid_box <- plot(mp_classif_rf, mp_classif_glm, mp_classif_svm, mp_classif_decTree, mp_classif_nnet, mp_classif_xgbTree,
     #mp_classif_logicBag, 
     geom = "boxplot") +
  theme_minimal() +
        theme(legend.position = 'bottom',
              plot.title = element_text(hjust = 0.5)) 
resid_box
```
```{r}
require(gridExtra)
grid.arrange(resid_box,resid_dist, ncol=2)
```

### Lift Curves

Lift curves describe a performance coefficient (lift) over the cumulative proportion of a population. Lift is calculated as the ratio of "yes's" on a certain sample point (for toxicity) divided by the ratio of "yes's" on the whole dataset. $Lift = Predicted Rate/Average Rate$. 


```{r eval=FALSE, include=FALSE}
#### DO NOT RUN - MAKES WEIRD PLOT###

# #recode class as character
# df2 <- df %>% 
#  mutate(effect = factor(case_when( #convert ordinal to numeric
#       effect_10 == "1" ~ "Y",
#       effect_10 == "0" ~ "N"
#     )))
# #split test/train
# set.seed(4)
# df2_split <- df2 %>%
#   initial_split(prop = 0.75)
# train2 <- training(df2_split)
# test2 <- testing(df2_split)
# 
# #set controls
# ctrl <- trainControl(method = "cv", classProbs = TRUE,
#                      summaryFunction = twoClassSummary)
# #create models for lift plots
# lift_rf <- train(effect ~., data = train2, method = "rf", ntree = 100, tuneLength = 1,
#                     trControl = ctrl)
# 
# lift_glm <- train(effect~., data = train2, method = "glm", family = "binomial", 
#                      trControl = ctrl)
# 
# lift_svm <- train(effect~., data = train2, method = "svmRadial", prob.model = TRUE, 
#                      tuneLength = 1, trControl = ctrl)#,
#                     # preProcess = c("pca","scale","center"))
# 
# lift_decTree <- train(effect~., data = train2, method = "C5.0", 
#                          preProcess =c("scale","center"), trControl = ctrl) 
# 
# lift_nnet <- train(effect~., data = train2, method = "nnet", preProcess=c("scale","center"),
#                       trControl = ctrl)
# 
# lift_xgbTree <- train(effect~., data = train2, method = "xgbTree", trControl = ctrl)
# 
# 
# ## Generate the test set results
# lift_results <- data.frame(effect = test2$effect)
# lift_results$rf <- predict(lift_rf, test, type = "prob")[,"Y"]
# lift_results$glm <- predict(lift_glm, test, type = "prob")[,"Y"]
# lift_results$svm <- predict(lift_svm, test, type = "prob")[,"Y"]
# lift_results$decTree <- predict(lift_decTree, test, type = "prob")[,"Y"]
# lift_results$nnet <- predict(lift_nnet, test, type = "prob")[,"Y"]
# lift_results$xgbTree <- predict(lift_xgbTree, test, type = "prob")[,"Y"]
# head(lift_results)
# 
# #plot results
# trellis.par.set(caretTheme())
# lift_obj <- lift(effect ~ rf + glm + svm + decTree + nnet + xgbTree, data = lift_results)
# plot(lift_obj, values = 60, auto.key = list(columns = 3,
#                                             lines = TRUE,
#                                             points = FALSE))
```
##### ALT METHOD
http://rstudio-pubs-static.s3.amazonaws.com/436131_3212dcf341cc422590f1a9f52830cfd6.html

```{r}
# transform datasets and model objects into scored data and calculate deciles 
scores_and_ntiles <- prepare_scores_and_ntiles(datasets=list("train","test"),
                                               dataset_labels = list("train data","test data"),
                                               models = list("classif_rf", "classif_decTree",
                                                             "classif_glm", "classif_nnet",
                                                             "classif_svm", "classif_xgbTree"),
                                               model_labels = list("random forest", "Decision
                                                                   tree", "General linear model",
                                                                   "Neural net", "Support Vector
                                                                   machine", "eXtreme Gradient
                                                                   Boosting Trees"),
                                               target_column="effect_10",
                                               ntiles = 100)


# transform data generated with prepare_scores_and_deciles into aggregated data for chosen plotting scope 
plot_input <- plotting_scope(prepared_input = scores_and_ntiles, scope = 'compare_models')
plot_cumgains(data = plot_input, custom_line_colors = RColorBrewer::brewer.pal(2,'Accent'))
```

```{r}
plot_cumlift(data = plot_input,custom_line_colors = RColorBrewer::brewer.pal(2,'Accent'))
```
```{r}
plot_cumresponse(data = plot_input,highlight_ntile = 20, 
                 custom_line_colors = RColorBrewer::brewer.pal(2,'Accent'))
```
```{r}
plot_multiplot(data = plot_input,  custom_line_colors = RColorBrewer::brewer.pal(2,'Accent'))
```


### Roc Curves
```{r}
plot(mp_classif_rf, 
     #mp_classif_glm, 
     mp_classif_svm, 
     #mp_classif_decTree, 
     mp_classif_nnet,
     #mp_classif_xgbTree,
     geom = "roc") +
  ggtitle("ROC Curves - All Models",  
          paste("AUC_rf = ",round(mp_classif_rf$measures$auc,3), 
                paste("AUC_svm = ",round(mp_classif_svm$measures$auc,3)),
          paste("AUC_nnet = ",round(mp_classif_nnet$measures$auc,3))
          )) +
#,  "AUC_glm = 0.799  AUC_svm = 0.798 AUC_decTree = AUC_nnet = AUC_xgbTree = ") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

### Variable Importance
```{r fig.height=7, fig.width=5, warning=FALSE}
vi_classif_rf <- model_parts(explainer_classif_rf, loss_function = loss_root_mean_square)
vi_classif_glm <- model_parts(explainer_classif_glm, loss_function = loss_root_mean_square)
vi_classif_svm <- model_parts(explainer_classif_svm, loss_function = loss_root_mean_square)
vi_classif_decTree <- model_parts(explainer_classif_decTree, loss_function = loss_root_mean_square)
vi_classif_nnet <- model_parts(explainer_classif_nnet, loss_function = loss_root_mean_square)
vi_classif_xgbTree <- model_parts(explainer_classif_xgbTree, loss_function = loss_root_mean_square)
#vi_classif_logicBag <- model_parts(explainer_classif_logicBag, loss_function = loss_root_mean_square)

plot(vi_classif_rf, vi_classif_glm, vi_classif_svm)#, #vi_classif_logicBag)
```
```{r}
plot(vi_classif_decTree, vi_classif_nnet, vi_classif_xgbTree)
```
#### EBM
```{r}
ebm_show(ebm, "dose.surface.area.um2.mL.master")
```
```{r}
ebm_show(ebm, "dose.um3.mL.master")
```


### Partial Dependence Plot
```{r}
pdp_classif_rf  <- model_profile(explainer_classif_rf, variable = "dose.um3.mL.master", type = "partial")
pdp_classif_glm  <- model_profile(explainer_classif_glm, variable = "dose.um3.mL.master", type = "partial")
pdp_classif_svm  <- model_profile(explainer_classif_svm, variable = "dose.um3.mL.master", type = "partial")
pdp_classif_decTree  <- model_profile(explainer_classif_decTree, variable = "dose.um3.mL.master", type = "partial")
pdp_classif_nnet  <- model_profile(explainer_classif_nnet, variable = "dose.um3.mL.master", type = "partial")
pdp_classif_xgbTree  <- model_profile(explainer_classif_xgbTree, variable = "dose.um3.mL.master", type = "partial")
#pdp_classif_logicBag  <- model_profile(explainer_classif_logicBag, variable = "dose.um3.mL.master", type = "partial")

plot(pdp_classif_rf, pdp_classif_glm, pdp_classif_svm, pdp_classif_decTree, pdp_classif_nnet, pdp_classif_xgbTree)#, pdp_classif_logicBag)
```

```{r}
#Partial Dependence organism type
pdp_classif_rf  <- model_profile(explainer_classif_rf, variable = "organism.group", type = "partial")
pdp_classif_glm  <- model_profile(explainer_classif_glm, variable = "organism.group", type = "partial")
pdp_classif_svm  <- model_profile(explainer_classif_svm, variable = "organism.group", type = "partial")
pdp_classif_decTree  <- model_profile(explainer_classif_decTree, variable = "organism.group", type = "partial")
pdp_classif_nnet  <- model_profile(explainer_classif_nnet, variable = "organism.group", type = "partial")
pdp_classif_xgbTree  <- model_profile(explainer_classif_xgbTree, variable = "organism.group", type = "partial")
#pdp_classif_logicBag  <- model_profile(explainer_classif_logicBag, variable = "dose.um3.mL.master", type = "partial")


plot(pdp_classif_rf$agr_profiles, pdp_classif_glm$agr_profiles, pdp_classif_svm$agr_profiles, pdp_classif_decTree$agr_profiles, pdp_classif_nnet$agr_profiles, pdp_classif_xgbTree$agr_profiles) +
  ggtitle("Contrastive Partial Dependence Profiles", "") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```


##### Grouped Partial Dependence Plots
```{r include=FALSE}
plot(pdp_classif_rf)
#Partial Dependence by dose.surface.area.um2.mL.master and bio.org
pdp_classif_rf  <- model_profile(explainer_classif_rf, variable = "dose.surface.area.um2.mL.master", 
                                 groups = "bio.org")
pdp_classif_glm  <- model_profile(explainer_classif_glm, variable = "dose.surface.area.um2.mL.master", 
                                 groups = "bio.org")
pdp_classif_svm  <- model_profile(explainer_classif_svm, variable = "dose.surface.area.um2.mL.master", 
                                 groups = "bio.org")
pdp_classif_decTree  <- model_profile(explainer_classif_decTree, variable = "dose.surface.area.um2.mL.master", 
                                 groups = "bio.org")
pdp_classif_nnet  <- model_profile(explainer_classif_nnet, variable = "dose.surface.area.um2.mL.master", 
                                 groups = "bio.org")
pdp_classif_xgbTree  <- model_profile(explainer_classif_xgbTree, variable = "dose.surface.area.um2.mL.master", 
                                 groups = "bio.org")
```

```{r}
#plot neural net and glm
glm <- plot(pdp_classif_glm$agr_profiles) +
  ggtitle("GLM: Partial Dependence Profiles by Dose and Level of Biological Organization", "") +
  xlab("dose.surface.area.um2.mL.master)") +
  ylab("Average Prediction for Effect (1 = yes, 0 = no)") +
  scale_color_discrete(name = "Model x Level of Biological Organization") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

nnet <- plot(pdp_classif_nnet$agr_profiles) +
  ggtitle("Neural Net: Partial Dependence Profiles by Dose and Level of Biological Organization", "") +
  xlab("dose.surface.area.um2.mL.master") +
  ylab("Average Prediction for Effect (1 = yes, 0 = no)") +
  scale_color_discrete(name = "Model x Level of Biological Organization") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
grid.arrange(glm, nnet)
```

```{r}
#plot rf and decision tree
rf <- plot(pdp_classif_rf$agr_profiles) +
  ggtitle("Random Forest:Partial Dependence Profiles by Dose and Level of Biological Organization", "") +
  xlab("dose.surface.area.um2.mL.master") +
  ylab("Average Prediction for Effect (1 = yes, 0 = no)") +
  scale_color_discrete(name = "Model x Level of Biological Organization") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

decTree <- plot(pdp_classif_decTree$agr_profiles) +
  ggtitle("Decision Tree: Partial Dependence Profiles by Dose and Level of Biological Organization", "") +
  xlab("dose.surface.area.um2.mL.master") +
  ylab("Average Prediction for Effect (1 = yes, 0 = no)") +
  scale_color_discrete(name = "Model x Level of Biological Organization") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
grid.arrange(rf, decTree)
```

```{r}
#plot rf and decision tree
svm <- plot(pdp_classif_svm$agr_profiles) +
  ggtitle("Support Vector Machine:Partial Dependence Profiles by Dose and Level of Biological Organization", "") +
  xlab("dose.surface.area.um2.mL.master") +
  ylab("Average Prediction for Effect (1 = yes, 0 = no)") +
  scale_color_discrete(name = "Model x Level of Biological Organization") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

xgbTree <- plot(pdp_classif_xgbTree$agr_profiles) +
  ggtitle("eXtreme Gradient Boosing Trees: Partial Dependence Profiles by Dose and Level of Biological Organization", "") +
  xlab("dose.surface.area.um2.mL.master") +
  ylab("Average Prediction for Effect (1 = yes, 0 = no)") +
  scale_color_discrete(name = "Model x Level of Biological Organization") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
grid.arrange(svm, xgbTree)
```

### Confusion Matrices

```{r include=FALSE}
#predict test data
pred_rf <- predict(classif_rf, newdata = test)
pred_glm <- predict(classif_glm, newdata = test)
pred_svm <- predict(classif_svm, newdata = test)
pred_decTree <- predict(classif_decTree, newdata = test)
pred_nnet <- predict(classif_nnet, newdata = test)
pred_xgbTree <- predict(classif_xgbTree, newdata = test)

#build confusion matrices
table_rf <- data.frame(confusionMatrix(pred_rf, test$effect_10)$table)
table_glm <- data.frame(confusionMatrix(pred_glm, test$effect_10)$table)
table_svm <- data.frame(confusionMatrix(pred_svm, test$effect_10)$table)
table_decTree <- data.frame(confusionMatrix(pred_decTree, test$effect_10)$table)
table_nnet <- data.frame(confusionMatrix(pred_nnet, test$effect_10)$table)
table_xgbTree <- data.frame(confusionMatrix(pred_xgbTree, test$effect_10)$table)

#build plots
plotTable_rf <- table_rf %>% mutate(goodbad = ifelse(table_rf$Prediction == table_rf$Reference, "good", "bad")) %>%
  group_by(Reference) %>% mutate(prop = Freq/sum(Freq))

plotTable_glm <- table_glm %>% mutate(goodbad = ifelse(table_glm$Prediction == table_glm$Reference, "good", "bad")) %>%
  group_by(Reference) %>% mutate(prop = Freq/sum(Freq))

plotTable_svm <- table_svm %>% mutate(goodbad = ifelse(table_svm$Prediction == table_svm$Reference, "good", "bad")) %>%
  group_by(Reference) %>% mutate(prop = Freq/sum(Freq))

plotTable_decTree <- table_decTree %>% mutate(goodbad = ifelse(table_decTree$Prediction == table_decTree$Reference, "good", "bad")) %>%
  group_by(Reference) %>% mutate(prop = Freq/sum(Freq))

plotTable_nnet <- table_nnet %>% mutate(goodbad = ifelse(table_nnet$Prediction == table_nnet$Reference, "good", "bad")) %>%
  group_by(Reference) %>% mutate(prop = Freq/sum(Freq))

plotTable_xgbTree <- table_xgbTree %>% mutate(goodbad = ifelse(table_xgbTree$Prediction == table_xgbTree$Reference, "good", "bad")) %>%
  group_by(Reference) %>% mutate(prop = Freq/sum(Freq))
```

Confusion Matrices for the six tested models are below.
```{r}
# fill alpha relative to sensitivity/specificity by proportional outcomes within reference groups (see dplyr code above as well as original confusion matrix for comparison)
CM_rf <- ggplot(data = plotTable_rf, mapping = aes(x = Reference, y = Prediction, fill = prop)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = .5, fontface  = "bold", alpha = 1) +
  scale_fill_gradient(low = "white", high = "cyan4", name = "Proportion") +
  scale_x_discrete(labels = c("No Effect", "Effect")) +
  scale_y_discrete(labels = c("No Effect", "Effect")) +
  ggtitle("Random Forest",
          paste("Accuracy = ", 100 * round(mp_classif_rf$measures$accuracy,3), "%")) +
  theme_bw() +
    theme(plot.title = element_text(hjust = 0.5),
          legend.position = "none")

CM_glm <- ggplot(data = plotTable_glm, mapping = aes(x = Reference, y = Prediction, fill = prop)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = .5, fontface  = "bold", alpha = 1) +
  scale_fill_gradient(low = "white", high = "cyan4", name = "Proportion") +
  scale_x_discrete(labels = c("No Effect", "Effect")) +
  scale_y_discrete(labels = c("No Effect", "Effect")) +
  ggtitle("General Linear Model",
          paste("Accuracy = ", 100 * round(mp_classif_glm$measures$accuracy,3), "%")) +
  theme_bw() +
    theme(plot.title = element_text(hjust = 0.5),
          legend.position = "none")

CM_svm <- ggplot(data = plotTable_svm, mapping = aes(x = Reference, y = Prediction, fill = prop)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = .5, fontface  = "bold", alpha = 1) +
  scale_fill_gradient(low = "white", high = "cyan4", name = "Proportion") +
  scale_x_discrete(labels = c("No Effect", "Effect")) +
  scale_y_discrete(labels = c("No Effect", "Effect")) +
  ggtitle("Support Vector Machine",
          paste("Accuracy = ", 100 * round(mp_classif_svm$measures$accuracy,3), "%")) +
  theme_bw() +
 theme(plot.title = element_text(hjust = 0.5),
          legend.position = "none")

CM_decTree <- ggplot(data = plotTable_decTree, mapping = aes(x = Reference, y = Prediction, fill = prop)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = .5, fontface  = "bold", alpha = 1) +
  scale_fill_gradient(low = "white", high = "cyan4", name = "Proportion") +
  scale_x_discrete(labels = c("No Effect", "Effect")) +
  scale_y_discrete(labels = c("No Effect", "Effect")) +
  ggtitle("Decision Tree",
          paste("Accuracy = ", 100 * round(mp_classif_decTree$measures$accuracy,3), "%")) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
          legend.position = "none")

CM_nnet <- ggplot(data = plotTable_nnet, mapping = aes(x = Reference, y = Prediction, fill = prop)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = .5, fontface  = "bold", alpha = 1) +
  scale_fill_gradient(low = "white", high = "cyan4", name = "Proportion") +
  scale_x_discrete(labels = c("No Effect", "Effect")) +
  scale_y_discrete(labels = c("No Effect", "Effect")) +
  ggtitle("Neural Net",
          paste("Accuracy = ", 100 * round(mp_classif_nnet$measures$accuracy,3), "%")) +
  theme_bw() +
    theme(plot.title = element_text(hjust = 0.5),
          legend.position = "none")

CM_xgbTree <- ggplot(data = plotTable_xgbTree, mapping = aes(x = Reference, y = Prediction, fill = prop)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = .5, fontface  = "bold", alpha = 1) +
  scale_fill_gradient(low = "white", high = "cyan4", name = "Proportion") +
  scale_x_discrete(labels = c("No Effect", "Effect")) +
  scale_y_discrete(labels = c("No Effect", "Effect")) +
  ggtitle("eXtreme Gradient Boosting Trees",
          paste("Accuracy = ", 100 * round(mp_classif_xgbTree$measures$accuracy,3), "%")) +
  theme_bw() +
    theme(plot.title = element_text(hjust = 0.5),
          legend.position = "none")

grid.arrange(CM_rf, CM_glm, CM_svm, CM_decTree, CM_nnet, CM_xgbTree,
             ncol = 2, top = "Confusion Matrices for ML Models")
```

### Breakdown

```{r}
# create a single observation
new_cust <- test$effect_10 %>% as.data.frame()


# compute breakdown distances
new_cust_glm <- predict_parts(explainer_classif_glm, new_observation = test, type = "break_down")
new_cust_rf <- predict_parts(explainer_classif_rf, new_observation = test, type = "break_down")
new_cust_svm <- predict_parts(explainer_classif_svm, new_observation = test, type = "break_down")
new_cust_decTree <- predict_parts(explainer_classif_decTree, new_observation = test, type = "break_down")
new_cust_nnet <- predict_parts(explainer_classif_nnet, new_observation = test, type = "break_down")
new_cust_xgbTree <- predict_parts(explainer_classif_xgbTree, new_observation = test, type = "break_down")
#new_cust_logicBag <- predict_parts(explainer_classif_logicBag, new_observation = test, type = "break_down")

# class of prediction_breakdown output
class(new_cust_rf)

# check out the top 10 influential variables for this observation
new_cust_rf[1:10, 1:5]
```

```{r}
plot(new_cust_glm, new_cust_rf, new_cust_svm, new_cust_decTree, new_cust_nnet, new_cust_xgbTree)#,
     #new_cust_logicBag)
```
```{r}
library(ggplot2)

# filter for top 10 influential variables for each model and plot
list(new_cust_glm, new_cust_rf) %>%
  purrr::map(~ top_n(., 11, wt = abs(contribution))) %>%
  do.call(rbind, .) %>%
  mutate(variable = paste0(variable, " (", label, ")")) %>%
  ggplot(aes(contribution, reorder(variable, contribution))) +
  geom_point() +
  geom_vline(xintercept = 0, size = 3, color = "white") +
  facet_wrap(~ label, scales = "free_y", ncol = 1) +
  ylab(NULL)
```

```{r}
library(ggplot2)

# filter for top 10 influential variables for each model and plot
list(new_cust_svm, new_cust_decTree) %>%
  purrr::map(~ top_n(., 11, wt = abs(contribution))) %>%
  do.call(rbind, .) %>%
  mutate(variable = paste0(variable, " (", label, ")")) %>%
  ggplot(aes(contribution, reorder(variable, contribution))) +
  geom_point() +
  geom_vline(xintercept = 0, size = 3, color = "white") +
  facet_wrap(~ label, scales = "free_y", ncol = 1) +
  ylab(NULL)
```

```{r}
library(ggplot2)

# filter for top 10 influential variables for each model and plot
list(new_cust_nnet,new_cust_xgbTree) %>%
  purrr::map(~ top_n(., 11, wt = abs(contribution))) %>%
  do.call(rbind, .) %>%
  mutate(variable = paste0(variable, " (", label, ")")) %>%
  ggplot(aes(contribution, reorder(variable, contribution))) +
  geom_point() +
  geom_vline(xintercept = 0, size = 3, color = "white") +
  facet_wrap(~ label, scales = "free_y", ncol = 1) +
  ylab(NULL)
```
#Save Models

Final models are saved in the folder: ML Models/FinalModels
```{r}
saveRDS(classif_glm, "ML Models/FinalModels/glm.rds")
saveRDS(classif_nnet, "ML Models/FinalModels/neuralNet.rds")
saveRDS(ebm, file="ML Models/ebm_model.rds")
saveRDS(classif_decTree, "ML Models/decisionTree.rds")
```


# Random Forest Tuning
All in all random forest is my final model of choice: it appears the more balanced and is the most accurate overall. This model will now be tuned and refined for maximum performance.

## Data selection

First, let's feed the random forest model as many variables as we believe to be reasonable. Later, we will perform recursive feature selection to allow the model to show us which variables are best predictors.
```{r}
kitchenSink <- aoc_z %>% 
  filter(tech.tier.zero == "Pass") %>% #gives studies that pass technical quality criteria
  #filter(risk.tier.zero == "Pass") %>%  #only studies applicable for risk assessment. VERY RESTRICTIVE 
  dplyr::select(c(organism.group, 
                  exposure.duration.d, 
                  lvl1_f, 
                  dose.um3.mL.master, 
                  dose.mg.L.master, 
                  dose.particles.mL.master, 
                  life.stage,
                  bio.org, 
                  polymer, 
                  shape,
                  size.length.um.used.for.conversions, 
                  treatments, 
                  effect, 
                  effect.metric,
                  particle.volume.um3,
                  density.mg.um.3,
                  environment,
                  exposure.route,
                  dose.surface.area.um2.mL.master,
                  lvl2_f
                  )) %>% 
  mutate(effect.metric = as.character(effect.metric)) %>% 
  mutate(effect.metric = (case_when(
    effect.metric == "NONEC" ~ "NOEC",
    effect.metric == "HONEC" ~ "NOEC",
    effect.metric == "LOEC" ~ "LOEC",
    effect.metric == "LC50" ~ "LC50",
    effect.metric == "EC50" ~"EC50",
    effect.metric == "EC10" ~ "EC10"
  ))) %>% 
  mutate(effect.metric = replace_na(effect.metric,"not_available")) %>% 
  #filter(effect.metric != "not_available") %>% 
  mutate(effect.metric = as.factor(effect.metric)) %>% 
  drop_na() %>%  #drop missing
mutate_if(~is.numeric(.) && (.) > 0, log10) %>% 
  mutate(effect_10 = case_when( #convert ordinal to numeric
      effect == "Y" ~ 1,
      effect == "N" ~ 0
    )) %>%
  mutate(effect_10 = factor(effect_10)) %>% 
  dplyr::select(-c(effect, effect.metric))

#ensure completeness
skim(kitchenSink)
```

```{r}
# create train, validation, and test splits
# Create calibration and validation splits with tidymodels initial_split() function.
set.seed(4)
ks_split <- kitchenSink %>%
  initial_split(prop = 0.75)
# default is 3/4ths split (but 75% training, 25% testing).

# Create a training data set with the training() function
# Pulls from training and testing sets created by initial_split()
train_ks <- training(ks_split)
test_ks <- testing(ks_split)

# variable names for response & features
y <- "effect_10"
x <- setdiff(names(kitchenSink), y) 

## subset for ebm syntax
train_x_ks <- as.data.frame(train_ks %>% dplyr::select(-effect_10))
train_y_ks <- as.numeric(as.vector(train_ks$effect_10))

test_x_ks <- as.data.frame(test_ks %>% dplyr::select(-effect_10))
test_y_ks <- as.numeric(as.vector(test_ks$effect_10))
```

## Tuning
http://rstudio-pubs-static.s3.amazonaws.com/480890_237ad52b09b6440e9c849a3c07a04d2f.html
```{r Tuning, include=FALSE}
cache = TRUE
set.seed(1000)
train_control <- trainControl(method = "repeatedcv", 
                              number = 10,
                              repeats = 5,
                              verboseIter = TRUE,
                              allowParallel = TRUE,
                              summaryFunction = multiClassSummary)
cache = TRUE
set.seed(1000)

start_time <- Sys.time() # Start timer

my_grid1 <- expand.grid(mtry = 1:17)

rf1 <- train(effect_10 ~ .,
             data = train_ks,
             method = "rf",
             metric = "Accuracy",
             tuneGrid = my_grid1,
             trControl = train_control)
rf1
```

Plot tuning.

```{r}
cache = TRUE

my_plot <- function(model) {
    theme_set(theme_minimal())
    u <- model$results %>%
        select(mtry, Accuracy, Kappa, F1, Sensitivity,
              Specificity,Pos_Pred_Value, Neg_Pred_Value, 
               Precision, Recall, Detection_Rate) %>%
        gather(a, b, -mtry)
    
    u %>% ggplot(aes(mtry, b)) + geom_line() + geom_point() + 
        facet_wrap(~ a, scales = "free") + 
        labs(x = "Number of mtry", y = NULL, 
             title = "The Relationship between Model Performance and mtry")
}

rf1 %>% my_plot()
```


I will further refine this model using recursive feature elimination and compare accuracy with the other models.

```{r Recursive Feature Elimination}

my_ctrl <- rfeControl(functions = rfFuncs, #random forests
                      method = "repeatedcv",
                      verbose = FALSE,
                      repeats = 5,
                      returnResamp = "all")

rfProfile <- rfe(y = train_ks$effect_10, # set dependent variable
              x = train_ks %>% 
                dplyr::select(-effect_10),
              rfeControl = my_ctrl,
               size = c(1:2, 4, 6, 8, 10, 12, 13))
rfProfile
```

## Predictor Selection
The following variables are those that were picked in the final (most accurate) model. 
```{r}
#get variable names picked in the final model
predictors(rfProfile)
```
The first 6 models are shown below with their corresponding accuracy and kappa values. 
```{r}
head(rfProfile$resample)
```

```{r}
trellis.par.set(caretTheme())
#plot(rfProfile, type = c("g", "o"), metric = "Accuracy")
ggplot(rfProfile) + theme_bw()
```
```{r}
plot1 <- xyplot(rfProfile, type = c("g", "p", "smooth"), main = "Accuracy of individual resampling results")
plot2 <- densityplot(rfProfile, 
                     subset = Variables < 5, 
                     adjust = 1.25, 
                     as.table = TRUE, 
                     xlab = "Accuracy Estimates", 
                     pch = "|")
print(plot1, split=c(1,1,1,2), more=TRUE)
print(plot2, split=c(1,2,1,2))
```



### Final Predictor Selection
```{r}
my_size <- pickSizeTolerance(rfProfile$results, metric = "Accuracy", tol = 1, maximize = TRUE)
# higher tol (~10) gives you less variables
# lower tol (~1) gives you more variables - "I'd like the simplest model within 1% of the best model."
accuracy1 <- pickVars(rfProfile$variables, size = my_size)
accuracy1
```
A random forest model with the above four predictors is within 1% of the best model. If a more accurate model is desired, and data is available, additional factors can be included in the model. Shown below are factors that should be included for a model that is within 0.5% accuracy of the best model.

```{r}
my_size <- pickSizeTolerance(rfProfile$results, metric = "Accuracy", tol = 0.5, maximize = TRUE)
# higher tol (~10) gives you less variables
# lower tol (~1) gives you more variables - "I'd like the simplest model within 1% of the best model."
accuracy0.5 <- pickVars(rfProfile$variables, size = my_size)
accuracy0.5
```
Below is a table showing which variables are included in models of varying accuracies.

```{r}
my_size <- pickSizeTolerance(rfProfile$results, metric = "Accuracy", tol = 0.1, maximize = TRUE)
accuracy0.1 <- pickVars(rfProfile$variables, size = my_size)
my_size <- pickSizeTolerance(rfProfile$results, metric = "Accuracy", tol = 0.3, maximize = TRUE)
accuracy0.3 <- pickVars(rfProfile$variables, size = my_size)
my_size <- pickSizeTolerance(rfProfile$results, metric = "Accuracy", tol = 5, maximize = TRUE)
accuracy5 <- pickVars(rfProfile$variables, size = my_size)
my_size <- pickSizeTolerance(rfProfile$results, metric = "Accuracy", tol = 10, maximize = TRUE)
accuracy10 <- pickVars(rfProfile$variables, size = my_size)

varsTable = list('0.1%' =accuracy0.1, '1%' = accuracy1, '5%' = accuracy5, '10%' = accuracy10)

#$make padded dataframe
na.pad <- function(x,len){
    x[1:len]
}

makePaddedDataFrame <- function(l,...){
    maxlen <- max(sapply(l,length))
    data.frame(lapply(l,na.pad,len=maxlen),...)
}
#fancy print
kable(makePaddedDataFrame(
  list('Accuracy (0.1)' = accuracy0.1, 'Accuracy (0.3)' = accuracy0.3, "Accuracy (1)" = accuracy1, 'Accuracy (5)' = accuracy5, 'Accuracy (10)' = accuracy10)
  ),
      caption = "Variables to include in Random Forest to achieve Model Accuracy (within x% of best model)",
    footnote = "Random Forest Recursive Feature Elimination")
```
### Final Model Tuning
While the absolute best model contains 18 variables with an estimated accuracy of 93.89%, it is possible to achieve ~93% accuracy with just eight variables, as displayed in the above table. The advantage of including fewer variables is that less information is needed to accurately predict toxicity. The final model with these eight variables will be further refined through an iterative tuning process to determine the optimal number of variables to be randomly collected for sampling at each split (mtry), and the number of branches to grow after each split.Code for the following section is inspired from https://rpubs.com/phamdinhkhanh/389752. 
```{r}
final_df <-df %>% 
  dplyr::select(c(#effect.metric, 
                  lvl2_f, 
                  dose.surface.area.um2.mL.master, 
                  effect_10, 
                  dose.um3.mL.master, 
                  organism.group,
                  exposure.duration.d#, 
                  #treatments
                  )) %>% 
  droplevels()
 
#ensure completeness
skim(final_df)
```

```{r}
# create train, validation, and test splits
# Create calibration and validation splits with tidymodels initial_split() function.
set.seed(4)
df_final_split <- final_df %>%
  initial_split(prop = 0.75)
# default is 3/4ths split (but 75% training, 25% testing).

# Create a training data set with the training() function
# Pulls from training and testing sets created by initial_split()
train_final <- training(df_final_split)
test_final <- testing(df_final_split)

# variable names for resonse & features
y <- "effect_10"
x <- setdiff(names(final_df), y) 
```

The mtry parameter will be optimized below:

```{r}
tunegrid <- expand.grid(.mtry=(1:6))
                       # , .ntree=c(500, 1000, 1500, 2000, 2500)) #would like to optimize but can't figure out how!

fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeat 3 times
                           repeats = 3)
                        
nrow(tunegrid)

set.seed(825)
tuneFit <- train(effect_10 ~ ., data = train_final, 
                 method = "rf", 
                 trControl = fitControl, 
                 verbose = FALSE, 
                 ## Now specify the exact models 
                 ## to evaluate:
                 metric = 'Accuracy',
                 tuneGrid = tunegrid)
tuneFit
```

```{r}
plot(tuneFit)
```
Now that mtry is optimized, the number of trees will be optimized, holding mtry constant.

```{r}
# Manual Search
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
tunegrid <- expand.grid(.mtry=c(7)) #replace with optimal value from previous chunk
modellist <- list()
for (ntree in c(1000, 1500, 2000, 2500)) {
	set.seed(123)
	fit <- train(effect_10~., data=train_final, method="rf", metric='Accuracy', 
	             tuneGrid=tunegrid, trControl=control, ntree=ntree)
	key <- toString(ntree)
	modellist[[key]] <- fit
}
# compare results
results <- resamples(modellist)
summary(results)
```

```{r}
dotplot(results)
```



### Final Model
Now that we have optimized the variables for our final model, we will build and save the final simplified model that has the highest accuracy.
```{r Final Model}
tunegrid <- expand.grid(.mtry=c(6))

final_model <- train(effect_10~., data = train_final, method = "rf", ntree = 2500, tuneLength = 5)

# explain
yTest <- as.numeric(as.character(test_final$effect_10))

explainer_final_model <- DALEX::explain(final_model, label = "rf",
                                       data = test_final, 
                                       y = yTest)

mp_classif_final_model <- model_performance(explainer_final_model)
mp_classif_final_model
```

```{r final model save}
#save model
saveRDS(final_rf_model, file="ML Models/FinalModels/final_rf_model.rds")
#load model
# final_model <- readRDS("final_rf_model.rds")
```

#### Performance
Performance metrics for the final model are below.

##### ROC Curve

```{r}
plot(mp_classif_final_model,
     geom = "roc") +
  ggtitle("ROC Curve - Final Model",  
          paste("AUC = ",round(mp_classif_final_model$measures$auc,2),
                paste("Accuracy = ", 100 * round(mp_classif_final_model$measures$accuracy,3), "%")
          )) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        legend.position = "none")
```

##### Confusion Matrix
The tuned, final, simplified model is now validated using test set data.
```{r}
test_pred <- predict(final_model, newdata = test_final)
confusionMatrix(table(test_pred, test_final$effect_10))
```
This confusion matrix is plotted below.
```{r}
table <- data.frame(confusionMatrix(test_pred, test_final$effect_10)$table)

plotTable <- table %>%
  mutate(goodbad = ifelse(table$Prediction == table$Reference, "good", "bad")) %>%
  group_by(Reference) %>%
  mutate(prop = Freq/sum(Freq))

# fill alpha relative to sensitivity/specificity by proportional outcomes within reference groups (see dplyr code above as well as original confusion matrix for comparison)
ggplot(data = plotTable, mapping = aes(x = Reference, y = Prediction, fill = prop)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = .5, fontface  = "bold", alpha = 1) +
  scale_fill_gradient(low = "white", high = "cyan4", name = "Proportion") +
  scale_x_discrete(labels = c("No Effect", "Effect")) +
  scale_y_discrete(labels = c("No Effect", "Effect")) +
  theme_bw()
```

##### What parameters bias the model?
Test residuals for size, dose, etc.

# Decision Tree

Decision trees are highly interpretable. An alternative package will be used to build and display a decision tree.

```{r, echo = FALSE}
#get column index of predicted variable in dataset
typeColNum <- grep("effect",names(all))

#build tree
require(rpart)
set.seed(15097)
t1 <- rpart(effect_10 ~ dose.surface.area.um2.mL.master + lvl2_f + 
              #exposure.route + 
              effect.metric +
              dose.um3.mL.master + bio.org + organism.group,
            method = "class", #classification because response is discrete
            control = rpart.control(minbucket = 20, cp=0.008), #requires that there be at least 19 cases (responded + nonrespondents) in the final grouping of variable values of the terminal node of the tre..
            data = train)
print(t1, digits=4)
```


Plot an interpretable tree.
```{r, echo = FALSE}
require(rpart.plot)
cols <- ifelse(t1$frame$yval == 1, "gray50", "black")
prp(t1, main="Tree for Effect",
    extra=106, # display prob of survival and percent of obs
    nn=TRUE, # display node numbers
    fallen.leaves=TRUE, # put leaves on the bottom of page
    branch=.5, # change angle of branch lines
    faclen=0, # do not abbreviate factor levels
    trace=1, # print automatically calculated cex
    shadow.col="gray", # shadows under the leaves
    branch.lty=1, # draw branches using solid lines
    branch.type=5, # branch lines width = weight(frame$wt), no. of cases here
    split.cex=1.2, # make split text larger than node text
    split.prefix="is ", # put "is " before split text
    split.suffix="?", # put "?" after split text
    col=cols, border.col=cols, # cols[2] if survived
    split.box.col="lightgray", # lightgray split boxes (default is white)
    split.border.col="darkgray", # darkgray border on split boxes
    split.round=0.5) # round the split box corners a tad
```
## Confusion matrix
```{r}
#make prediction
t1_pred <- predict(t1, test, type = "class")
#build confusion matrix
confMat <- data.frame(confusionMatrix(t1_pred, test$effect_10)$table)
#get accuracy
table <- table(test$effect_10, t1_pred)
decTreeAccuracy <- sum(diag(table))/sum(table)

plotTable <- confMat %>%
  mutate(goodbad = ifelse(confMat$Prediction == confMat$Reference, "good", "bad")) %>%
  group_by(Reference) %>%
  mutate(prop = Freq/sum(Freq))

# fill alpha relative to sensitivity/specificity by proportional outcomes within reference groups (see dplyr code above as well as original confusion matrix for comparison)
ggplot(data = plotTable, mapping = aes(x = Reference, y = Prediction, fill = prop)) +
  geom_tile() +
  ggtitle(paste("Decision Tree Confusion Matrix (Accuracy = ", 100 * round(decTreeAccuracy,3), "%)")) +
  geom_text(aes(label = Freq), vjust = .5, fontface  = "bold", alpha = 1) +
  scale_fill_gradient(low = "white", high = "cyan4", name = "Proportion") +
  scale_x_discrete(labels = c("No Effect", "Effect")) +
  scale_y_discrete(labels = c("No Effect", "Effect")) +
  theme_bw()
```


# Machine Settings
Time to knit:
```{r}
all_times
```


Machine Info:
```{r}
Sys.info()[c(1:3,5)]
```


Session Info:
```{r}
sessionInfo()
```